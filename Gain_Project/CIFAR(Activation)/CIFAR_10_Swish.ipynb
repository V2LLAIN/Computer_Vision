{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-04-15 06:01:25.572801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 06:01:25.654563: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 06:01:25.676469: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6274e9cb-9c04-457a-a71c-1fa4b824c728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# Swish Activation Function\n",
    "def swish(x):\n",
    "    return K.sigmoid(x) * x\n",
    "\n",
    "# Swish Layer\n",
    "class swish(Activation):\n",
    "    def __init__(self, activation=swish, **kwargs):\n",
    "        super(swish, self).__init__(activation, **kwargs)\n",
    "        self.activation = activation\n",
    "        self.__name__ = 'swish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        network = Activation('swish')(network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              strides=1, activation='swish', normalize_batch=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 50:\n",
    "        pass\n",
    "    elif epoch > 50 and epoch <= 100:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 06:01:27.532597: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.534435: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.534522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.534881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 06:01:27.535448: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.535560: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.535630: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.899408: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.899545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.899624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 06:01:27.899698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10093 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['add[0][0]',                    \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['add_1[0][0]',                  \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['add_2[0][0]',                  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['add_3[0][0]',                  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['add_4[0][0]',                  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['add_5[0][0]',                  \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   4640        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 32)   544         ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 16, 32)   0           ['add_7[0][0]',                  \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['add_8[0][0]',                  \n",
      "                                                                  'batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['add_9[0][0]',                  \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['add_10[0][0]',                 \n",
      "                                                                  'batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 32)   0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['add_11[0][0]',                 \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['add_12[0][0]',                 \n",
      "                                                                  'batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 64)     18496       ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 64)     2112        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 64)     36928       ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['add_14[0][0]',                 \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     36928       ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['add_15[0][0]',                 \n",
      "                                                                  'batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     36928       ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['add_16[0][0]',                 \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 8, 8, 64)     0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['add_17[0][0]',                 \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 8, 8, 64)     0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['add_18[0][0]',                 \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 8, 64)     0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['add_19[0][0]',                 \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_22[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/wandb/run-20230415_060129-cl0dfswd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR10/runs/cl0dfswd' target=\"_blank\">ResNet-ReLU</a></strong> to <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">https://wandb.ai/hcim/CIFAR10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR10/runs/cl0dfswd' target=\"_blank\">https://wandb.ai/hcim/CIFAR10/runs/cl0dfswd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR10/runs/cl0dfswd?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa98c1c2d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR10\", entity=\"hcim\", name='ResNet-Swish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 06:01:34.256906: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2023-04-15 06:01:35.281116: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 27s 57ms/step - loss: 2.5747 - accuracy: 0.1379 - val_loss: 53.2665 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0834 - accuracy: 0.3291 - val_loss: 5.3374 - val_accuracy: 0.1737 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.7954 - accuracy: 0.4422 - val_loss: 1.6681 - val_accuracy: 0.5084 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.6001 - accuracy: 0.5159 - val_loss: 1.4756 - val_accuracy: 0.5723 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 21s 55ms/step - loss: 1.4250 - accuracy: 0.5803 - val_loss: 1.5445 - val_accuracy: 0.5561 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.3204 - accuracy: 0.6188 - val_loss: 3.4303 - val_accuracy: 0.1864 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2324 - accuracy: 0.6479 - val_loss: 2.6810 - val_accuracy: 0.3302 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1597 - accuracy: 0.6741 - val_loss: 1.8968 - val_accuracy: 0.5114 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.1220 - accuracy: 0.6888 - val_loss: 1.1350 - val_accuracy: 0.6907 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.0768 - accuracy: 0.7041 - val_loss: 3.2705 - val_accuracy: 0.2681 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.0433 - accuracy: 0.7179 - val_loss: 3.5037 - val_accuracy: 0.2438 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 1.0072 - accuracy: 0.7306 - val_loss: 1.9733 - val_accuracy: 0.4928 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9816 - accuracy: 0.7392 - val_loss: 0.8738 - val_accuracy: 0.7825 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9694 - accuracy: 0.7413 - val_loss: 1.1431 - val_accuracy: 0.7068 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.9429 - accuracy: 0.7533 - val_loss: 1.0773 - val_accuracy: 0.7174 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9398 - accuracy: 0.7542 - val_loss: 1.3383 - val_accuracy: 0.6500 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9201 - accuracy: 0.7608 - val_loss: 20.9811 - val_accuracy: 0.1019 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9099 - accuracy: 0.7650 - val_loss: 1.3252 - val_accuracy: 0.6555 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8941 - accuracy: 0.7731 - val_loss: 1.0802 - val_accuracy: 0.7186 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.8849 - accuracy: 0.7757 - val_loss: 0.9683 - val_accuracy: 0.7671 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.8831 - accuracy: 0.7747 - val_loss: 1.4116 - val_accuracy: 0.6583 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8662 - accuracy: 0.7839 - val_loss: 1.3867 - val_accuracy: 0.6443 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8611 - accuracy: 0.7837 - val_loss: 2.2936 - val_accuracy: 0.5144 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8577 - accuracy: 0.7873 - val_loss: 1.0968 - val_accuracy: 0.7122 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8500 - accuracy: 0.7913 - val_loss: 1.5506 - val_accuracy: 0.5798 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 26/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8416 - accuracy: 0.7938 - val_loss: 0.7876 - val_accuracy: 0.8214 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 27/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8419 - accuracy: 0.7951 - val_loss: 0.9265 - val_accuracy: 0.7748 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 28/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8354 - accuracy: 0.7983 - val_loss: 9.9775 - val_accuracy: 0.1102 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 29/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8339 - accuracy: 0.7998 - val_loss: 1.1210 - val_accuracy: 0.7233 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 30/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8311 - accuracy: 0.8000 - val_loss: 0.9753 - val_accuracy: 0.7625 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8282 - accuracy: 0.8031 - val_loss: 2.0272 - val_accuracy: 0.4800 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8239 - accuracy: 0.8047 - val_loss: 2.0368 - val_accuracy: 0.4846 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8151 - accuracy: 0.8096 - val_loss: 1.2876 - val_accuracy: 0.6917 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8223 - accuracy: 0.8048 - val_loss: 1.0816 - val_accuracy: 0.7473 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8196 - accuracy: 0.8073 - val_loss: 1.3189 - val_accuracy: 0.6939 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8087 - accuracy: 0.8112 - val_loss: 0.8654 - val_accuracy: 0.8105 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8067 - accuracy: 0.8134 - val_loss: 4.8212 - val_accuracy: 0.2637 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7990 - accuracy: 0.8158 - val_loss: 0.8409 - val_accuracy: 0.8165 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8110 - accuracy: 0.8111 - val_loss: 1.1159 - val_accuracy: 0.7280 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 40/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7994 - accuracy: 0.8162 - val_loss: 0.8905 - val_accuracy: 0.7969 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 41/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7926 - accuracy: 0.8184 - val_loss: 0.9404 - val_accuracy: 0.7897 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 42/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8054 - accuracy: 0.8144 - val_loss: 1.8533 - val_accuracy: 0.5165 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 43/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7907 - accuracy: 0.8214 - val_loss: 1.3372 - val_accuracy: 0.6927 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 44/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7915 - accuracy: 0.8199 - val_loss: 0.9168 - val_accuracy: 0.8009 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7957 - accuracy: 0.8196 - val_loss: 1.2963 - val_accuracy: 0.7048 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7918 - accuracy: 0.8218 - val_loss: 0.8053 - val_accuracy: 0.8265 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8014 - accuracy: 0.8187 - val_loss: 0.9432 - val_accuracy: 0.7916 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7977 - accuracy: 0.8207 - val_loss: 0.9232 - val_accuracy: 0.8009 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7870 - accuracy: 0.8257 - val_loss: 1.4180 - val_accuracy: 0.6425 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7903 - accuracy: 0.8241 - val_loss: 1.1715 - val_accuracy: 0.7263 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.7827 - accuracy: 0.8253 - val_loss: 3.5286 - val_accuracy: 0.2299 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.6772 - accuracy: 0.8624 - val_loss: 0.7254 - val_accuracy: 0.8551 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.6157 - accuracy: 0.8841 - val_loss: 0.5743 - val_accuracy: 0.8991 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5930 - accuracy: 0.8895 - val_loss: 0.5891 - val_accuracy: 0.8942 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 55/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5755 - accuracy: 0.8933 - val_loss: 0.5776 - val_accuracy: 0.8970 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 56/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5677 - accuracy: 0.8955 - val_loss: 0.5665 - val_accuracy: 0.8992 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 57/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5606 - accuracy: 0.8950 - val_loss: 0.5180 - val_accuracy: 0.9131 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 58/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5476 - accuracy: 0.8992 - val_loss: 0.5181 - val_accuracy: 0.9142 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 59/180\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.5332 - accuracy: 0.9037 - val_loss: 0.5137 - val_accuracy: 0.9130 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 60/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5305 - accuracy: 0.9030 - val_loss: 0.5294 - val_accuracy: 0.9073 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 61/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5195 - accuracy: 0.9055 - val_loss: 0.5574 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 62/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5129 - accuracy: 0.9074 - val_loss: 0.4980 - val_accuracy: 0.9158 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 63/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5094 - accuracy: 0.9065 - val_loss: 0.5130 - val_accuracy: 0.9116 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 64/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.5004 - accuracy: 0.9093 - val_loss: 0.4916 - val_accuracy: 0.9175 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4931 - accuracy: 0.9105 - val_loss: 0.5074 - val_accuracy: 0.9093 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4873 - accuracy: 0.9111 - val_loss: 0.5069 - val_accuracy: 0.9132 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4773 - accuracy: 0.9138 - val_loss: 0.4925 - val_accuracy: 0.9158 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4813 - accuracy: 0.9107 - val_loss: 0.5002 - val_accuracy: 0.9114 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4691 - accuracy: 0.9136 - val_loss: 0.5042 - val_accuracy: 0.9101 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4613 - accuracy: 0.9156 - val_loss: 0.6454 - val_accuracy: 0.8700 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4597 - accuracy: 0.9154 - val_loss: 0.4992 - val_accuracy: 0.9093 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4529 - accuracy: 0.9172 - val_loss: 0.5281 - val_accuracy: 0.9036 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4467 - accuracy: 0.9175 - val_loss: 0.5339 - val_accuracy: 0.9000 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4437 - accuracy: 0.9177 - val_loss: 0.4851 - val_accuracy: 0.9138 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4404 - accuracy: 0.9194 - val_loss: 0.4851 - val_accuracy: 0.9109 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4342 - accuracy: 0.9193 - val_loss: 0.4754 - val_accuracy: 0.9136 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4304 - accuracy: 0.9193 - val_loss: 0.4802 - val_accuracy: 0.9132 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4257 - accuracy: 0.9213 - val_loss: 0.5494 - val_accuracy: 0.8906 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4233 - accuracy: 0.9208 - val_loss: 0.5223 - val_accuracy: 0.8996 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4175 - accuracy: 0.9213 - val_loss: 0.4981 - val_accuracy: 0.9087 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4157 - accuracy: 0.9216 - val_loss: 0.5495 - val_accuracy: 0.8906 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4133 - accuracy: 0.9205 - val_loss: 0.4784 - val_accuracy: 0.9123 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4084 - accuracy: 0.9233 - val_loss: 0.4483 - val_accuracy: 0.9207 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4013 - accuracy: 0.9250 - val_loss: 0.5422 - val_accuracy: 0.8914 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4005 - accuracy: 0.9242 - val_loss: 0.4860 - val_accuracy: 0.9071 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.4016 - accuracy: 0.9243 - val_loss: 0.4598 - val_accuracy: 0.9147 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3942 - accuracy: 0.9254 - val_loss: 0.4505 - val_accuracy: 0.9170 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3913 - accuracy: 0.9253 - val_loss: 0.4952 - val_accuracy: 0.9020 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 89/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3914 - accuracy: 0.9248 - val_loss: 0.4730 - val_accuracy: 0.9086 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 90/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3884 - accuracy: 0.9237 - val_loss: 0.4399 - val_accuracy: 0.9168 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 91/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3853 - accuracy: 0.9265 - val_loss: 0.4683 - val_accuracy: 0.9127 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 92/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3834 - accuracy: 0.9257 - val_loss: 0.4314 - val_accuracy: 0.9194 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 93/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3804 - accuracy: 0.9274 - val_loss: 0.5980 - val_accuracy: 0.8739 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3779 - accuracy: 0.9260 - val_loss: 0.4500 - val_accuracy: 0.9136 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3749 - accuracy: 0.9264 - val_loss: 0.5647 - val_accuracy: 0.8823 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3748 - accuracy: 0.9258 - val_loss: 0.4472 - val_accuracy: 0.9163 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3701 - accuracy: 0.9274 - val_loss: 0.4710 - val_accuracy: 0.9080 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3729 - accuracy: 0.9266 - val_loss: 0.5199 - val_accuracy: 0.8945 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3690 - accuracy: 0.9271 - val_loss: 0.4416 - val_accuracy: 0.9139 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3637 - accuracy: 0.9283 - val_loss: 0.4858 - val_accuracy: 0.9027 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3620 - accuracy: 0.9288 - val_loss: 0.4280 - val_accuracy: 0.9178 - lr: 0.0100\n",
      "Learning rate: 0.001\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3432 - accuracy: 0.9357 - val_loss: 0.4094 - val_accuracy: 0.9223 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3316 - accuracy: 0.9399 - val_loss: 0.4040 - val_accuracy: 0.9235 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3287 - accuracy: 0.9400 - val_loss: 0.4039 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 105/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3209 - accuracy: 0.9436 - val_loss: 0.3996 - val_accuracy: 0.9255 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 106/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3222 - accuracy: 0.9434 - val_loss: 0.4005 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 107/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3151 - accuracy: 0.9451 - val_loss: 0.3989 - val_accuracy: 0.9265 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 108/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3167 - accuracy: 0.9455 - val_loss: 0.4064 - val_accuracy: 0.9248 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3152 - accuracy: 0.9433 - val_loss: 0.3983 - val_accuracy: 0.9254 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3113 - accuracy: 0.9464 - val_loss: 0.3952 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 111/180\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.3118 - accuracy: 0.9452 - val_loss: 0.4006 - val_accuracy: 0.9263 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 112/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3096 - accuracy: 0.9470 - val_loss: 0.4013 - val_accuracy: 0.9253 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 113/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3082 - accuracy: 0.9474 - val_loss: 0.3960 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 114/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3074 - accuracy: 0.9478 - val_loss: 0.4057 - val_accuracy: 0.9251 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 115/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3067 - accuracy: 0.9469 - val_loss: 0.4006 - val_accuracy: 0.9277 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3064 - accuracy: 0.9476 - val_loss: 0.4067 - val_accuracy: 0.9257 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3055 - accuracy: 0.9484 - val_loss: 0.4000 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 118/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3017 - accuracy: 0.9495 - val_loss: 0.4067 - val_accuracy: 0.9267 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 119/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3007 - accuracy: 0.9493 - val_loss: 0.4067 - val_accuracy: 0.9260 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 120/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2970 - accuracy: 0.9507 - val_loss: 0.3983 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 121/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2997 - accuracy: 0.9505 - val_loss: 0.4013 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 122/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2985 - accuracy: 0.9495 - val_loss: 0.4007 - val_accuracy: 0.9278 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3006 - accuracy: 0.9493 - val_loss: 0.4003 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3002 - accuracy: 0.9494 - val_loss: 0.3969 - val_accuracy: 0.9293 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2968 - accuracy: 0.9512 - val_loss: 0.3994 - val_accuracy: 0.9280 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2918 - accuracy: 0.9525 - val_loss: 0.3966 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.3000 - accuracy: 0.9484 - val_loss: 0.3984 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2943 - accuracy: 0.9510 - val_loss: 0.3947 - val_accuracy: 0.9299 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2952 - accuracy: 0.9501 - val_loss: 0.3948 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 130/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2922 - accuracy: 0.9510 - val_loss: 0.3989 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 131/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2947 - accuracy: 0.9502 - val_loss: 0.4149 - val_accuracy: 0.9239 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 132/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2930 - accuracy: 0.9499 - val_loss: 0.3960 - val_accuracy: 0.9290 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 133/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2909 - accuracy: 0.9520 - val_loss: 0.3968 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 134/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2928 - accuracy: 0.9511 - val_loss: 0.3949 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 135/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2917 - accuracy: 0.9510 - val_loss: 0.4050 - val_accuracy: 0.9259 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 136/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2947 - accuracy: 0.9492 - val_loss: 0.3944 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 137/180\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.2887 - accuracy: 0.9525 - val_loss: 0.3947 - val_accuracy: 0.9292 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 138/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2919 - accuracy: 0.9509 - val_loss: 0.3945 - val_accuracy: 0.9308 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 139/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2878 - accuracy: 0.9522 - val_loss: 0.4024 - val_accuracy: 0.9268 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 140/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2893 - accuracy: 0.9525 - val_loss: 0.3986 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 141/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2896 - accuracy: 0.9521 - val_loss: 0.4040 - val_accuracy: 0.9274 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 142/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2896 - accuracy: 0.9519 - val_loss: 0.3950 - val_accuracy: 0.9300 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 143/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2896 - accuracy: 0.9511 - val_loss: 0.4015 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 144/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2853 - accuracy: 0.9531 - val_loss: 0.3927 - val_accuracy: 0.9293 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 145/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2910 - accuracy: 0.9494 - val_loss: 0.3995 - val_accuracy: 0.9265 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 146/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2876 - accuracy: 0.9521 - val_loss: 0.3991 - val_accuracy: 0.9275 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 147/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2839 - accuracy: 0.9541 - val_loss: 0.3992 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 148/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2845 - accuracy: 0.9532 - val_loss: 0.4003 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 149/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2818 - accuracy: 0.9543 - val_loss: 0.3973 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 150/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2817 - accuracy: 0.9542 - val_loss: 0.3960 - val_accuracy: 0.9290 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 151/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2824 - accuracy: 0.9541 - val_loss: 0.3973 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 152/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2807 - accuracy: 0.9544 - val_loss: 0.3993 - val_accuracy: 0.9282 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 153/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2835 - accuracy: 0.9526 - val_loss: 0.3962 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 154/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2783 - accuracy: 0.9551 - val_loss: 0.3960 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 155/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2814 - accuracy: 0.9523 - val_loss: 0.3979 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 156/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2793 - accuracy: 0.9549 - val_loss: 0.3975 - val_accuracy: 0.9283 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 157/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2811 - accuracy: 0.9534 - val_loss: 0.3930 - val_accuracy: 0.9295 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 158/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2741 - accuracy: 0.9552 - val_loss: 0.4027 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 159/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2767 - accuracy: 0.9548 - val_loss: 0.3948 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 160/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2759 - accuracy: 0.9537 - val_loss: 0.3919 - val_accuracy: 0.9293 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 161/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2742 - accuracy: 0.9563 - val_loss: 0.3954 - val_accuracy: 0.9290 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2765 - accuracy: 0.9546 - val_loss: 0.3913 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.2750 - accuracy: 0.9550 - val_loss: 0.3938 - val_accuracy: 0.9289 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2774 - accuracy: 0.9542 - val_loss: 0.3942 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2770 - accuracy: 0.9544 - val_loss: 0.4063 - val_accuracy: 0.9275 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2748 - accuracy: 0.9551 - val_loss: 0.3982 - val_accuracy: 0.9293 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2729 - accuracy: 0.9552 - val_loss: 0.3940 - val_accuracy: 0.9270 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2760 - accuracy: 0.9544 - val_loss: 0.3923 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 169/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2710 - accuracy: 0.9556 - val_loss: 0.3918 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 170/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2714 - accuracy: 0.9567 - val_loss: 0.3985 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 171/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2729 - accuracy: 0.9554 - val_loss: 0.3988 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 172/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2692 - accuracy: 0.9570 - val_loss: 0.3961 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 173/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2729 - accuracy: 0.9549 - val_loss: 0.3995 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 174/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2696 - accuracy: 0.9560 - val_loss: 0.3950 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 175/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2698 - accuracy: 0.9563 - val_loss: 0.3986 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 176/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2705 - accuracy: 0.9560 - val_loss: 0.3947 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 177/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2661 - accuracy: 0.9565 - val_loss: 0.3907 - val_accuracy: 0.9299 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 178/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2684 - accuracy: 0.9571 - val_loss: 0.3951 - val_accuracy: 0.9282 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 179/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2704 - accuracy: 0.9558 - val_loss: 0.3934 - val_accuracy: 0.9290 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.2648 - accuracy: 0.9582 - val_loss: 0.3908 - val_accuracy: 0.9277 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faa2d715a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e946-b5c5-48d8-90e6-1144be24915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
