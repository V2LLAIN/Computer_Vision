{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:33:54.779795: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 11:33:54.903553: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe35a65-5dc2-438b-b26b-c6a0a6bf9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "\n",
    "# SwiCh Activation Function\n",
    "def swich(x):\n",
    "    return tf.where(x>=0, x, x * (tf.sigmoid(x) * (tf.exp(x) + 1)))\n",
    "\n",
    "# SwiCh Layer\n",
    "class Swich(Activation):\n",
    "    def __init__(self, activation=swich, **kwargs):\n",
    "        super(Swich, self).__init__(activation, **kwargs)\n",
    "        self.activation = activation\n",
    "        self.__name__ = 'swich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        network = Activation(swish)(network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              strides=1, activation=swish, normalize_batch=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 50:\n",
    "        pass\n",
    "    elif epoch > 50 and epoch <= 100:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:33:57.122926: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.125508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.125811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.126663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 11:33:57.127266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.127576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.127738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.600437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.600624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.600770: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 11:33:57.600901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10400 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['add[0][0]',                    \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['add_1[0][0]',                  \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['add_2[0][0]',                  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['add_3[0][0]',                  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['add_4[0][0]',                  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['add_5[0][0]',                  \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   4640        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 32)   544         ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 16, 32)   0           ['add_7[0][0]',                  \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['add_8[0][0]',                  \n",
      "                                                                  'batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['add_9[0][0]',                  \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['add_10[0][0]',                 \n",
      "                                                                  'batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 32)   0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['add_11[0][0]',                 \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['add_12[0][0]',                 \n",
      "                                                                  'batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 64)     18496       ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 64)     2112        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 64)     36928       ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['add_14[0][0]',                 \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     36928       ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['add_15[0][0]',                 \n",
      "                                                                  'batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     36928       ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['add_16[0][0]',                 \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 8, 8, 64)     0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['add_17[0][0]',                 \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 8, 8, 64)     0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['add_18[0][0]',                 \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 8, 64)     0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['add_19[0][0]',                 \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_22[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/gain_proj/wandb/run-20230415_113400-zggllslj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR10/runs/zggllslj' target=\"_blank\">ResNet-SwiCh</a></strong> to <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">https://wandb.ai/hcim/CIFAR10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR10/runs/zggllslj' target=\"_blank\">https://wandb.ai/hcim/CIFAR10/runs/zggllslj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR10/runs/zggllslj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f2994639100>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR10\", entity=\"hcim\", name='ResNet-SwiCh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 11:34:04.989692: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 34s 77ms/step - loss: 2.6497 - accuracy: 0.0999 - val_loss: 2.6009 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 2.5803 - accuracy: 0.1009 - val_loss: 2.5595 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 2.3197 - accuracy: 0.2094 - val_loss: 2.6685 - val_accuracy: 0.0859 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.9185 - accuracy: 0.3694 - val_loss: 6.4441 - val_accuracy: 0.1088 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 1.6395 - accuracy: 0.4812 - val_loss: 3.0103 - val_accuracy: 0.2230 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.4383 - accuracy: 0.5598 - val_loss: 1.8626 - val_accuracy: 0.5028 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 1.2925 - accuracy: 0.6139 - val_loss: 1.6421 - val_accuracy: 0.5102 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.1839 - accuracy: 0.6565 - val_loss: 3.2312 - val_accuracy: 0.1970 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.1166 - accuracy: 0.6799 - val_loss: 2.0199 - val_accuracy: 0.3638 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 30s 75ms/step - loss: 1.0538 - accuracy: 0.7034 - val_loss: 2.3490 - val_accuracy: 0.2467 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.0165 - accuracy: 0.7180 - val_loss: 3.8509 - val_accuracy: 0.2088 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.9839 - accuracy: 0.7311 - val_loss: 4.2588 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9501 - accuracy: 0.7449 - val_loss: 3.9877 - val_accuracy: 0.1413 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 0.9418 - accuracy: 0.7495 - val_loss: 4.3431 - val_accuracy: 0.1333 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.9150 - accuracy: 0.7607 - val_loss: 3.1334 - val_accuracy: 0.2861 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 30s 76ms/step - loss: 1.0105 - accuracy: 0.7297 - val_loss: 3.6321 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 30s 75ms/step - loss: 0.9231 - accuracy: 0.7598 - val_loss: 3.0199 - val_accuracy: 0.2881 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.8873 - accuracy: 0.7747 - val_loss: 1.4215 - val_accuracy: 0.6079 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.8775 - accuracy: 0.7786 - val_loss: 3.7288 - val_accuracy: 0.1500 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.8595 - accuracy: 0.7843 - val_loss: 7.3510 - val_accuracy: 0.1475 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.8504 - accuracy: 0.7876 - val_loss: 3.3925 - val_accuracy: 0.2284 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.8367 - accuracy: 0.7926 - val_loss: 2.0369 - val_accuracy: 0.5213 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 30s 77ms/step - loss: 0.8229 - accuracy: 0.7979 - val_loss: 1.2678 - val_accuracy: 0.6872 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.8159 - accuracy: 0.8029 - val_loss: 1.5269 - val_accuracy: 0.5954 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.8156 - accuracy: 0.8031 - val_loss: 1.3607 - val_accuracy: 0.6415 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 26/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.8033 - accuracy: 0.8094 - val_loss: 1.2471 - val_accuracy: 0.6863 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 27/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.7962 - accuracy: 0.8114 - val_loss: 2.9853 - val_accuracy: 0.2576 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 28/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7936 - accuracy: 0.8120 - val_loss: 1.5063 - val_accuracy: 0.6268 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 29/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.7960 - accuracy: 0.8131 - val_loss: 0.9698 - val_accuracy: 0.7703 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 30/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7897 - accuracy: 0.8148 - val_loss: 1.9805 - val_accuracy: 0.4706 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.7771 - accuracy: 0.8181 - val_loss: 1.1315 - val_accuracy: 0.7284 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7747 - accuracy: 0.8207 - val_loss: 1.2069 - val_accuracy: 0.6827 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.7779 - accuracy: 0.8187 - val_loss: 0.9971 - val_accuracy: 0.7605 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7769 - accuracy: 0.8226 - val_loss: 1.1750 - val_accuracy: 0.7213 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7722 - accuracy: 0.8247 - val_loss: 4.0342 - val_accuracy: 0.2178 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.7567 - accuracy: 0.8291 - val_loss: 1.4160 - val_accuracy: 0.6750 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.7611 - accuracy: 0.8271 - val_loss: 1.6248 - val_accuracy: 0.5981 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7569 - accuracy: 0.8315 - val_loss: 1.0422 - val_accuracy: 0.7530 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7619 - accuracy: 0.8294 - val_loss: 1.0370 - val_accuracy: 0.7490 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 40/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7497 - accuracy: 0.8336 - val_loss: 1.4828 - val_accuracy: 0.6216 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 41/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7590 - accuracy: 0.8307 - val_loss: 1.7626 - val_accuracy: 0.4934 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 42/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7669 - accuracy: 0.8286 - val_loss: 1.7980 - val_accuracy: 0.5990 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 43/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.7498 - accuracy: 0.8366 - val_loss: 2.7180 - val_accuracy: 0.4272 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 44/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7494 - accuracy: 0.8359 - val_loss: 2.4684 - val_accuracy: 0.2524 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7502 - accuracy: 0.8359 - val_loss: 3.1964 - val_accuracy: 0.3295 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7496 - accuracy: 0.8353 - val_loss: 1.1023 - val_accuracy: 0.7592 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7448 - accuracy: 0.8380 - val_loss: 3.5702 - val_accuracy: 0.2968 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7493 - accuracy: 0.8369 - val_loss: 2.5698 - val_accuracy: 0.2660 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7427 - accuracy: 0.8398 - val_loss: 0.7356 - val_accuracy: 0.8501 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.7409 - accuracy: 0.8396 - val_loss: 0.7651 - val_accuracy: 0.8375 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.7336 - accuracy: 0.8433 - val_loss: 0.9014 - val_accuracy: 0.7939 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.6422 - accuracy: 0.8747 - val_loss: 0.6077 - val_accuracy: 0.8888 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.5711 - accuracy: 0.8980 - val_loss: 0.5615 - val_accuracy: 0.9048 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.5545 - accuracy: 0.9006 - val_loss: 0.5710 - val_accuracy: 0.8990 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 55/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.5431 - accuracy: 0.9039 - val_loss: 0.5319 - val_accuracy: 0.9125 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 56/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.5280 - accuracy: 0.9088 - val_loss: 0.5741 - val_accuracy: 0.8948 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 57/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.5172 - accuracy: 0.9104 - val_loss: 0.5398 - val_accuracy: 0.9068 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 58/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.5039 - accuracy: 0.9135 - val_loss: 0.5166 - val_accuracy: 0.9140 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 59/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4980 - accuracy: 0.9155 - val_loss: 0.6163 - val_accuracy: 0.8832 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 60/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4884 - accuracy: 0.9170 - val_loss: 0.5450 - val_accuracy: 0.9035 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 61/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.4801 - accuracy: 0.9182 - val_loss: 0.5346 - val_accuracy: 0.9063 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 62/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.4728 - accuracy: 0.9203 - val_loss: 0.5429 - val_accuracy: 0.9008 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 63/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4683 - accuracy: 0.9195 - val_loss: 0.5393 - val_accuracy: 0.9031 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 64/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4627 - accuracy: 0.9206 - val_loss: 0.7003 - val_accuracy: 0.8610 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4561 - accuracy: 0.9207 - val_loss: 0.4889 - val_accuracy: 0.9176 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4495 - accuracy: 0.9226 - val_loss: 0.5813 - val_accuracy: 0.8872 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4430 - accuracy: 0.9236 - val_loss: 0.4897 - val_accuracy: 0.9167 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4315 - accuracy: 0.9262 - val_loss: 0.4789 - val_accuracy: 0.9188 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4278 - accuracy: 0.9278 - val_loss: 0.4885 - val_accuracy: 0.9140 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4256 - accuracy: 0.9276 - val_loss: 0.4894 - val_accuracy: 0.9144 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4224 - accuracy: 0.9272 - val_loss: 0.4942 - val_accuracy: 0.9119 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4189 - accuracy: 0.9276 - val_loss: 0.5200 - val_accuracy: 0.9056 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4133 - accuracy: 0.9293 - val_loss: 0.5124 - val_accuracy: 0.9027 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.4094 - accuracy: 0.9294 - val_loss: 0.5033 - val_accuracy: 0.9062 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.4024 - accuracy: 0.9305 - val_loss: 0.4788 - val_accuracy: 0.9136 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3976 - accuracy: 0.9312 - val_loss: 0.4726 - val_accuracy: 0.9164 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3895 - accuracy: 0.9331 - val_loss: 0.4545 - val_accuracy: 0.9198 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.3930 - accuracy: 0.9316 - val_loss: 0.4794 - val_accuracy: 0.9132 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3842 - accuracy: 0.9352 - val_loss: 0.6010 - val_accuracy: 0.8708 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.3856 - accuracy: 0.9319 - val_loss: 0.4740 - val_accuracy: 0.9152 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3811 - accuracy: 0.9332 - val_loss: 0.4777 - val_accuracy: 0.9100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.3733 - accuracy: 0.9349 - val_loss: 0.4528 - val_accuracy: 0.9192 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3764 - accuracy: 0.9334 - val_loss: 0.5646 - val_accuracy: 0.8881 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3680 - accuracy: 0.9357 - val_loss: 0.5559 - val_accuracy: 0.8876 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.3679 - accuracy: 0.9356 - val_loss: 0.4575 - val_accuracy: 0.9157 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3652 - accuracy: 0.9354 - val_loss: 0.4371 - val_accuracy: 0.9222 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3569 - accuracy: 0.9368 - val_loss: 0.6525 - val_accuracy: 0.8695 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3590 - accuracy: 0.9360 - val_loss: 0.5701 - val_accuracy: 0.8819 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 89/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3528 - accuracy: 0.9370 - val_loss: 0.4401 - val_accuracy: 0.9173 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 90/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.3491 - accuracy: 0.9375 - val_loss: 0.4446 - val_accuracy: 0.9159 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 91/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.3485 - accuracy: 0.9380 - val_loss: 0.5101 - val_accuracy: 0.8978 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 92/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3496 - accuracy: 0.9371 - val_loss: 0.4775 - val_accuracy: 0.9025 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 93/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3440 - accuracy: 0.9373 - val_loss: 0.4737 - val_accuracy: 0.9084 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3433 - accuracy: 0.9366 - val_loss: 0.5186 - val_accuracy: 0.8951 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3337 - accuracy: 0.9399 - val_loss: 0.4744 - val_accuracy: 0.9074 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3364 - accuracy: 0.9397 - val_loss: 0.5255 - val_accuracy: 0.8939 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3303 - accuracy: 0.9410 - val_loss: 0.4318 - val_accuracy: 0.9193 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3390 - accuracy: 0.9377 - val_loss: 0.5140 - val_accuracy: 0.8946 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3347 - accuracy: 0.9380 - val_loss: 0.4680 - val_accuracy: 0.9078 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3331 - accuracy: 0.9390 - val_loss: 0.4240 - val_accuracy: 0.9184 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.3247 - accuracy: 0.9412 - val_loss: 0.4418 - val_accuracy: 0.9185 - lr: 0.0100\n",
      "Learning rate: 0.001\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.3089 - accuracy: 0.9459 - val_loss: 0.4029 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2975 - accuracy: 0.9511 - val_loss: 0.4006 - val_accuracy: 0.9268 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2938 - accuracy: 0.9523 - val_loss: 0.4073 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 105/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2926 - accuracy: 0.9532 - val_loss: 0.3958 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 106/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2880 - accuracy: 0.9549 - val_loss: 0.3996 - val_accuracy: 0.9264 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 107/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2858 - accuracy: 0.9557 - val_loss: 0.4080 - val_accuracy: 0.9231 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 108/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2832 - accuracy: 0.9560 - val_loss: 0.4101 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2844 - accuracy: 0.9558 - val_loss: 0.4026 - val_accuracy: 0.9247 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2844 - accuracy: 0.9547 - val_loss: 0.3975 - val_accuracy: 0.9278 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 111/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2802 - accuracy: 0.9558 - val_loss: 0.3946 - val_accuracy: 0.9277 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 112/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2802 - accuracy: 0.9567 - val_loss: 0.3989 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 113/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2780 - accuracy: 0.9570 - val_loss: 0.3987 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 114/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2783 - accuracy: 0.9568 - val_loss: 0.3972 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 115/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2750 - accuracy: 0.9576 - val_loss: 0.3956 - val_accuracy: 0.9286 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2775 - accuracy: 0.9562 - val_loss: 0.4005 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2741 - accuracy: 0.9578 - val_loss: 0.4146 - val_accuracy: 0.9249 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 118/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2744 - accuracy: 0.9578 - val_loss: 0.3927 - val_accuracy: 0.9295 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 119/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2755 - accuracy: 0.9577 - val_loss: 0.4073 - val_accuracy: 0.9264 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 120/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2698 - accuracy: 0.9593 - val_loss: 0.3980 - val_accuracy: 0.9272 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 121/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2693 - accuracy: 0.9597 - val_loss: 0.4049 - val_accuracy: 0.9271 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 122/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2670 - accuracy: 0.9603 - val_loss: 0.3983 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2705 - accuracy: 0.9588 - val_loss: 0.3930 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2674 - accuracy: 0.9602 - val_loss: 0.4011 - val_accuracy: 0.9275 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2647 - accuracy: 0.9612 - val_loss: 0.3956 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2692 - accuracy: 0.9584 - val_loss: 0.4015 - val_accuracy: 0.9273 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2673 - accuracy: 0.9598 - val_loss: 0.4000 - val_accuracy: 0.9280 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2647 - accuracy: 0.9608 - val_loss: 0.3998 - val_accuracy: 0.9271 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 32s 82ms/step - loss: 0.2678 - accuracy: 0.9591 - val_loss: 0.3992 - val_accuracy: 0.9274 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 130/180\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.2644 - accuracy: 0.9599 - val_loss: 0.3961 - val_accuracy: 0.9290 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 131/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2635 - accuracy: 0.9609 - val_loss: 0.4219 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 132/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.2632 - accuracy: 0.9611 - val_loss: 0.3937 - val_accuracy: 0.9292 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 133/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2634 - accuracy: 0.9602 - val_loss: 0.3957 - val_accuracy: 0.9294 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 134/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2634 - accuracy: 0.9597 - val_loss: 0.4038 - val_accuracy: 0.9266 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 135/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2637 - accuracy: 0.9597 - val_loss: 0.3940 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 136/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2583 - accuracy: 0.9624 - val_loss: 0.3944 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 137/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2606 - accuracy: 0.9609 - val_loss: 0.4029 - val_accuracy: 0.9263 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 138/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2574 - accuracy: 0.9619 - val_loss: 0.4058 - val_accuracy: 0.9259 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 139/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2580 - accuracy: 0.9615 - val_loss: 0.4027 - val_accuracy: 0.9278 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 140/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2584 - accuracy: 0.9614 - val_loss: 0.3975 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 141/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2608 - accuracy: 0.9605 - val_loss: 0.3956 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 142/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.2598 - accuracy: 0.9611 - val_loss: 0.3993 - val_accuracy: 0.9276 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 143/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2560 - accuracy: 0.9624 - val_loss: 0.3980 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 144/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2562 - accuracy: 0.9626 - val_loss: 0.4001 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 145/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2548 - accuracy: 0.9619 - val_loss: 0.3915 - val_accuracy: 0.9292 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 146/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2537 - accuracy: 0.9625 - val_loss: 0.3942 - val_accuracy: 0.9297 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 147/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2578 - accuracy: 0.9614 - val_loss: 0.3985 - val_accuracy: 0.9278 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 148/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2548 - accuracy: 0.9619 - val_loss: 0.4065 - val_accuracy: 0.9267 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 149/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2526 - accuracy: 0.9643 - val_loss: 0.3987 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 150/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2547 - accuracy: 0.9621 - val_loss: 0.3941 - val_accuracy: 0.9272 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 151/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.2529 - accuracy: 0.9630 - val_loss: 0.4121 - val_accuracy: 0.9255 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 152/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2538 - accuracy: 0.9624 - val_loss: 0.3916 - val_accuracy: 0.9304 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 153/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2514 - accuracy: 0.9632 - val_loss: 0.3971 - val_accuracy: 0.9275 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 154/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2524 - accuracy: 0.9631 - val_loss: 0.3988 - val_accuracy: 0.9274 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 155/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2538 - accuracy: 0.9623 - val_loss: 0.4102 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 156/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2495 - accuracy: 0.9640 - val_loss: 0.3977 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 157/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2520 - accuracy: 0.9632 - val_loss: 0.3900 - val_accuracy: 0.9296 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 158/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2511 - accuracy: 0.9632 - val_loss: 0.3992 - val_accuracy: 0.9275 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 159/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2522 - accuracy: 0.9621 - val_loss: 0.3953 - val_accuracy: 0.9284 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 160/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2503 - accuracy: 0.9634 - val_loss: 0.3875 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 161/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2473 - accuracy: 0.9639 - val_loss: 0.4004 - val_accuracy: 0.9265 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2469 - accuracy: 0.9646 - val_loss: 0.4000 - val_accuracy: 0.9279 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2476 - accuracy: 0.9635 - val_loss: 0.4002 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2493 - accuracy: 0.9639 - val_loss: 0.3965 - val_accuracy: 0.9267 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.2458 - accuracy: 0.9643 - val_loss: 0.3965 - val_accuracy: 0.9274 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2457 - accuracy: 0.9641 - val_loss: 0.3930 - val_accuracy: 0.9288 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 32s 80ms/step - loss: 0.2460 - accuracy: 0.9643 - val_loss: 0.3913 - val_accuracy: 0.9291 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2434 - accuracy: 0.9655 - val_loss: 0.3895 - val_accuracy: 0.9301 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 169/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2431 - accuracy: 0.9649 - val_loss: 0.4030 - val_accuracy: 0.9269 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 170/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2433 - accuracy: 0.9644 - val_loss: 0.3855 - val_accuracy: 0.9310 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 171/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2464 - accuracy: 0.9634 - val_loss: 0.3936 - val_accuracy: 0.9281 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 172/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2443 - accuracy: 0.9645 - val_loss: 0.3943 - val_accuracy: 0.9285 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 173/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2438 - accuracy: 0.9648 - val_loss: 0.4025 - val_accuracy: 0.9266 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 174/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2432 - accuracy: 0.9649 - val_loss: 0.4028 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 175/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2445 - accuracy: 0.9646 - val_loss: 0.3950 - val_accuracy: 0.9271 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 176/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2443 - accuracy: 0.9641 - val_loss: 0.3970 - val_accuracy: 0.9274 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 177/180\n",
      "391/391 [==============================] - 32s 81ms/step - loss: 0.2450 - accuracy: 0.9633 - val_loss: 0.3924 - val_accuracy: 0.9276 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 178/180\n",
      "391/391 [==============================] - 31s 78ms/step - loss: 0.2387 - accuracy: 0.9658 - val_loss: 0.3888 - val_accuracy: 0.9289 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 179/180\n",
      "391/391 [==============================] - 31s 80ms/step - loss: 0.2390 - accuracy: 0.9661 - val_loss: 0.3873 - val_accuracy: 0.9297 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 31s 79ms/step - loss: 0.2391 - accuracy: 0.9655 - val_loss: 0.3887 - val_accuracy: 0.9294 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29943401c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
