{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-04-15 13:48:58.193895: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 13:48:58.274257: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 13:48:58.296191: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=100)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fe35a65-5dc2-438b-b26b-c6a0a6bf9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "\n",
    "# SwiCh Activation Function\n",
    "def swich(x):\n",
    "    return tf.where(x>=0, x, x * (tf.sigmoid(x) * (tf.exp(x) + 1)))\n",
    "\n",
    "# SwiCh Layer\n",
    "class Swich(Activation):\n",
    "    def __init__(self, activation=swich, **kwargs):\n",
    "        super(Swich, self).__init__(activation, **kwargs)\n",
    "        self.activation = activation\n",
    "        self.__name__ = 'swich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=100):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        network = Activation(swish)(network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              strides=1, activation=swish, normalize_batch=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 50:\n",
    "        pass\n",
    "    elif epoch > 50 and epoch <= 100:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:49:00.108030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.110264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.110390: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.110710: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 13:49:00.111091: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.111180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.111248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.473137: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.473278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.473358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 13:49:00.473431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['add[0][0]',                    \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['add_1[0][0]',                  \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['add_2[0][0]',                  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['add_3[0][0]',                  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['add_4[0][0]',                  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['add_5[0][0]',                  \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   4640        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 32)   544         ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 16, 32)   0           ['add_7[0][0]',                  \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['add_8[0][0]',                  \n",
      "                                                                  'batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['add_9[0][0]',                  \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['add_10[0][0]',                 \n",
      "                                                                  'batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 32)   0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['add_11[0][0]',                 \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['add_12[0][0]',                 \n",
      "                                                                  'batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 64)     18496       ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 64)     2112        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 64)     36928       ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['add_14[0][0]',                 \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     36928       ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['add_15[0][0]',                 \n",
      "                                                                  'batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     36928       ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['add_16[0][0]',                 \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 8, 8, 64)     0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['add_17[0][0]',                 \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 8, 8, 64)     0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['add_18[0][0]',                 \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 8, 64)     0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['add_19[0][0]',                 \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_22[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          6500        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 669,092\n",
      "Trainable params: 667,300\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/wandb/run-20230415_134902-pjx2lvw1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR100/runs/pjx2lvw1' target=\"_blank\">ResNet-SwiCh</a></strong> to <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">https://wandb.ai/hcim/CIFAR100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR100/runs/pjx2lvw1' target=\"_blank\">https://wandb.ai/hcim/CIFAR100/runs/pjx2lvw1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR100/runs/pjx2lvw1?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4683d1cdf0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR100\", entity=\"hcim\", name='ResNet-SwiCh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 13:49:06.791289: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2023-04-15 13:49:07.805718: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 27s 58ms/step - loss: 4.9539 - accuracy: 0.0095 - val_loss: 4.9081 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.8885 - accuracy: 0.0089 - val_loss: 4.8670 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.8484 - accuracy: 0.0099 - val_loss: 4.8285 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.8140 - accuracy: 0.0094 - val_loss: 4.7971 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.7848 - accuracy: 0.0100 - val_loss: 4.7721 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.7595 - accuracy: 0.0098 - val_loss: 4.7514 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.7390 - accuracy: 0.0086 - val_loss: 4.7309 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.7206 - accuracy: 0.0098 - val_loss: 4.7150 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.7053 - accuracy: 0.0092 - val_loss: 4.7009 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.6919 - accuracy: 0.0099 - val_loss: 4.6885 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.6802 - accuracy: 0.0102 - val_loss: 4.6776 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.6706 - accuracy: 0.0094 - val_loss: 4.6682 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.6331 - accuracy: 0.0121 - val_loss: 181.6438 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 4.2435 - accuracy: 0.0493 - val_loss: 15.3862 - val_accuracy: 0.0104 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.9379 - accuracy: 0.0948 - val_loss: 6.3719 - val_accuracy: 0.0330 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.6842 - accuracy: 0.1389 - val_loss: 9.4944 - val_accuracy: 0.0117 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.4441 - accuracy: 0.1836 - val_loss: 3.6238 - val_accuracy: 0.1714 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.2531 - accuracy: 0.2247 - val_loss: 4.6293 - val_accuracy: 0.1044 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 3.0762 - accuracy: 0.2633 - val_loss: 4.0918 - val_accuracy: 0.1560 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.9167 - accuracy: 0.2981 - val_loss: 4.0828 - val_accuracy: 0.1721 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.7821 - accuracy: 0.3317 - val_loss: 3.6935 - val_accuracy: 0.2040 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.6799 - accuracy: 0.3584 - val_loss: 2.6315 - val_accuracy: 0.3720 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.6001 - accuracy: 0.3828 - val_loss: 2.3766 - val_accuracy: 0.4357 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.5201 - accuracy: 0.4016 - val_loss: 8.4551 - val_accuracy: 0.0738 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.2548 - accuracy: 0.4824 - val_loss: 2.9898 - val_accuracy: 0.3606 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.4588 - accuracy: 0.4528 - val_loss: 4.2100 - val_accuracy: 0.2047 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.6502 - accuracy: 0.4173 - val_loss: 5.9015 - val_accuracy: 0.1536 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.3199 - accuracy: 0.4858 - val_loss: 7.7923 - val_accuracy: 0.0812 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.2747 - accuracy: 0.4975 - val_loss: 2.3465 - val_accuracy: 0.4899 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.2226 - accuracy: 0.5108 - val_loss: 2.3235 - val_accuracy: 0.4922 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.2223 - accuracy: 0.5108 - val_loss: 2.9060 - val_accuracy: 0.3979 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.1945 - accuracy: 0.5210 - val_loss: 4.9148 - val_accuracy: 0.2129 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.1922 - accuracy: 0.5213 - val_loss: 3.1482 - val_accuracy: 0.3574 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.1069 - accuracy: 0.5448 - val_loss: 2.6453 - val_accuracy: 0.4673 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.1046 - accuracy: 0.5502 - val_loss: 4.0612 - val_accuracy: 0.2887 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0908 - accuracy: 0.5551 - val_loss: 12.9626 - val_accuracy: 0.0294 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0903 - accuracy: 0.5556 - val_loss: 2.3846 - val_accuracy: 0.5120 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0780 - accuracy: 0.5605 - val_loss: 2.2320 - val_accuracy: 0.5400 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0692 - accuracy: 0.5622 - val_loss: 2.7852 - val_accuracy: 0.4346 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0654 - accuracy: 0.5637 - val_loss: 2.9913 - val_accuracy: 0.4171 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 2.0643 - accuracy: 0.5632 - val_loss: 2.4141 - val_accuracy: 0.5126 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.7987 - accuracy: 0.6327 - val_loss: 1.7846 - val_accuracy: 0.6390 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.6700 - accuracy: 0.6623 - val_loss: 1.8411 - val_accuracy: 0.6267 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.6146 - accuracy: 0.6755 - val_loss: 1.7462 - val_accuracy: 0.6495 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 55/180\n",
      " 80/391 [=====>........................] - ETA: 15s - loss: 1.5814 - accuracy: 0.6855"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 22s 55ms/step - loss: 1.4111 - accuracy: 0.7126 - val_loss: 1.6471 - val_accuracy: 0.6614 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.4037 - accuracy: 0.7127 - val_loss: 1.7289 - val_accuracy: 0.6466 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3842 - accuracy: 0.7177 - val_loss: 1.6581 - val_accuracy: 0.6615 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3732 - accuracy: 0.7185 - val_loss: 1.9169 - val_accuracy: 0.6021 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3623 - accuracy: 0.7180 - val_loss: 1.6317 - val_accuracy: 0.6669 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3491 - accuracy: 0.7199 - val_loss: 1.7111 - val_accuracy: 0.6519 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3457 - accuracy: 0.7191 - val_loss: 1.6849 - val_accuracy: 0.6553 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3283 - accuracy: 0.7246 - val_loss: 1.5980 - val_accuracy: 0.6688 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3221 - accuracy: 0.7251 - val_loss: 1.6658 - val_accuracy: 0.6545 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3094 - accuracy: 0.7240 - val_loss: 1.6171 - val_accuracy: 0.6669 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.3020 - accuracy: 0.7275 - val_loss: 1.6929 - val_accuracy: 0.6487 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2874 - accuracy: 0.7309 - val_loss: 1.6136 - val_accuracy: 0.6681 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2785 - accuracy: 0.7324 - val_loss: 1.6454 - val_accuracy: 0.6570 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2810 - accuracy: 0.7308 - val_loss: 1.7830 - val_accuracy: 0.6348 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2662 - accuracy: 0.7320 - val_loss: 1.6228 - val_accuracy: 0.6670 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2523 - accuracy: 0.7351 - val_loss: 1.8426 - val_accuracy: 0.6187 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2420 - accuracy: 0.7379 - val_loss: 2.0361 - val_accuracy: 0.5875 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2306 - accuracy: 0.7397 - val_loss: 1.6667 - val_accuracy: 0.6482 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2298 - accuracy: 0.7399 - val_loss: 1.7197 - val_accuracy: 0.6443 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2314 - accuracy: 0.7353 - val_loss: 1.5851 - val_accuracy: 0.6685 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2172 - accuracy: 0.7411 - val_loss: 1.5635 - val_accuracy: 0.6733 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2083 - accuracy: 0.7419 - val_loss: 1.7056 - val_accuracy: 0.6400 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.2101 - accuracy: 0.7415 - val_loss: 1.7986 - val_accuracy: 0.6267 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1984 - accuracy: 0.7417 - val_loss: 1.6577 - val_accuracy: 0.6531 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1638 - accuracy: 0.7482 - val_loss: 1.7351 - val_accuracy: 0.6318 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1590 - accuracy: 0.7481 - val_loss: 1.8678 - val_accuracy: 0.6067 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1491 - accuracy: 0.7519 - val_loss: 1.6854 - val_accuracy: 0.6495 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1449 - accuracy: 0.7509 - val_loss: 1.7876 - val_accuracy: 0.6293 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1455 - accuracy: 0.7506 - val_loss: 1.6161 - val_accuracy: 0.6637 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1373 - accuracy: 0.7548 - val_loss: 1.7455 - val_accuracy: 0.6370 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1359 - accuracy: 0.7537 - val_loss: 1.6393 - val_accuracy: 0.6613 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1262 - accuracy: 0.7556 - val_loss: 1.8559 - val_accuracy: 0.6127 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.1251 - accuracy: 0.7571 - val_loss: 1.6171 - val_accuracy: 0.6678 - lr: 0.0100\n",
      "Learning rate: 0.001\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 1.0372 - accuracy: 0.7796 - val_loss: 1.5383 - val_accuracy: 0.6770 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9995 - accuracy: 0.7914 - val_loss: 1.5253 - val_accuracy: 0.6816 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9556 - accuracy: 0.8012 - val_loss: 1.5304 - val_accuracy: 0.6846 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9506 - accuracy: 0.8045 - val_loss: 1.5371 - val_accuracy: 0.6817 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9561 - accuracy: 0.8047 - val_loss: 1.5394 - val_accuracy: 0.6790 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 111/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9570 - accuracy: 0.8023 - val_loss: 1.5660 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 112/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9471 - accuracy: 0.8059 - val_loss: 1.5259 - val_accuracy: 0.6847 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 113/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9436 - accuracy: 0.8072 - val_loss: 1.5409 - val_accuracy: 0.6835 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 114/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9443 - accuracy: 0.8055 - val_loss: 1.5409 - val_accuracy: 0.6834 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 115/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9379 - accuracy: 0.8084 - val_loss: 1.5493 - val_accuracy: 0.6814 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9430 - accuracy: 0.8087 - val_loss: 1.5237 - val_accuracy: 0.6886 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9224 - accuracy: 0.8108 - val_loss: 1.5493 - val_accuracy: 0.6823 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9141 - accuracy: 0.8143 - val_loss: 1.5524 - val_accuracy: 0.6843 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9142 - accuracy: 0.8137 - val_loss: 1.5854 - val_accuracy: 0.6757 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9080 - accuracy: 0.8170 - val_loss: 1.5324 - val_accuracy: 0.6883 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9079 - accuracy: 0.8164 - val_loss: 1.5241 - val_accuracy: 0.6912 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9067 - accuracy: 0.8142 - val_loss: 1.5494 - val_accuracy: 0.6832 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9030 - accuracy: 0.8176 - val_loss: 1.5669 - val_accuracy: 0.6834 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9072 - accuracy: 0.8151 - val_loss: 1.5473 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 130/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9040 - accuracy: 0.8165 - val_loss: 1.5423 - val_accuracy: 0.6855 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 131/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.9036 - accuracy: 0.8153 - val_loss: 1.5335 - val_accuracy: 0.6871 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 132/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8963 - accuracy: 0.8187 - val_loss: 1.5565 - val_accuracy: 0.6816 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 133/180\n",
      "357/391 [==========================>...] - ETA: 1s - loss: 0.8990 - accuracy: 0.8175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8525 - accuracy: 0.8277 - val_loss: 1.5882 - val_accuracy: 0.6793 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 159/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8638 - accuracy: 0.8249 - val_loss: 1.5702 - val_accuracy: 0.6822 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 160/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8593 - accuracy: 0.8260 - val_loss: 1.5749 - val_accuracy: 0.6819 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 161/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8618 - accuracy: 0.8228 - val_loss: 1.5675 - val_accuracy: 0.6811 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8599 - accuracy: 0.8235 - val_loss: 1.5661 - val_accuracy: 0.6836 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.8550 - accuracy: 0.8263 - val_loss: 1.5521 - val_accuracy: 0.6845 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8548 - accuracy: 0.8264 - val_loss: 1.5723 - val_accuracy: 0.6832 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8549 - accuracy: 0.8255 - val_loss: 1.5666 - val_accuracy: 0.6849 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8500 - accuracy: 0.8279 - val_loss: 1.5540 - val_accuracy: 0.6880 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8488 - accuracy: 0.8262 - val_loss: 1.5504 - val_accuracy: 0.6869 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8431 - accuracy: 0.8297 - val_loss: 1.5594 - val_accuracy: 0.6861 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 169/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8413 - accuracy: 0.8292 - val_loss: 1.5758 - val_accuracy: 0.6833 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 170/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8462 - accuracy: 0.8260 - val_loss: 1.5793 - val_accuracy: 0.6816 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 171/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8432 - accuracy: 0.8286 - val_loss: 1.5765 - val_accuracy: 0.6825 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 172/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8377 - accuracy: 0.8286 - val_loss: 1.5690 - val_accuracy: 0.6834 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 173/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8407 - accuracy: 0.8278 - val_loss: 1.6194 - val_accuracy: 0.6720 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 174/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8292 - accuracy: 0.8321 - val_loss: 1.5628 - val_accuracy: 0.6844 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 22s 55ms/step - loss: 0.8287 - accuracy: 0.8312 - val_loss: 1.5705 - val_accuracy: 0.6841 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47547f4520>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
