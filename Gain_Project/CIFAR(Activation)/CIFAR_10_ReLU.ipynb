{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-04-14 09:52:06.302912: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 09:52:06.382793: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-14 09:52:06.404314: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 10\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=10):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        network = Activation('relu')(network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              strides=1, activation='relu', normalize_batch=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 50:\n",
    "        pass\n",
    "    elif epoch > 50 and epoch <= 100:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 32, 32, 16)   448         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 32, 32, 16)  64          ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 32, 32, 16)   0           ['conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_24[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 32, 32, 16)  64          ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 32, 32, 16)   0           ['activation_23[0][0]',          \n",
      "                                                                  'batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 32, 32, 16)   2320        ['add_21[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 32, 32, 16)   0           ['conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 32, 32, 16)  64          ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, 32, 32, 16)   0           ['add_21[0][0]',                 \n",
      "                                                                  'batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 32, 32, 16)   2320        ['add_22[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 32, 32, 16)   0           ['conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_26[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 32, 32, 16)  64          ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, 32, 32, 16)   0           ['add_22[0][0]',                 \n",
      "                                                                  'batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 32, 32, 16)   2320        ['add_23[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 32, 32, 16)   0           ['conv2d_52[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 32, 32, 16)  64          ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_24 (Add)                   (None, 32, 32, 16)   0           ['add_23[0][0]',                 \n",
      "                                                                  'batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 32, 32, 16)   2320        ['add_24[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 32, 32, 16)   0           ['conv2d_54[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 32, 32, 16)  64          ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_25 (Add)                   (None, 32, 32, 16)   0           ['add_24[0][0]',                 \n",
      "                                                                  'batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 32, 32, 16)   2320        ['add_25[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 32, 32, 16)   0           ['conv2d_56[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_29[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 32, 32, 16)  64          ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_26 (Add)                   (None, 32, 32, 16)   0           ['add_25[0][0]',                 \n",
      "                                                                  'batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 32, 32, 16)   2320        ['add_26[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 32, 32, 16)   0           ['conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_30[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 32, 32, 16)  64          ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_27 (Add)                   (None, 32, 32, 16)   0           ['add_26[0][0]',                 \n",
      "                                                                  'batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 16, 16, 32)   4640        ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 16, 16, 32)   0           ['conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 16, 16, 32)   544         ['add_27[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 16, 16, 32)  128         ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 16, 16, 32)  128         ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_28 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_33[0][0]', \n",
      "                                                                  'batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 16, 16, 32)   9248        ['add_28[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 16, 16, 32)   0           ['conv2d_63[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 16, 16, 32)  128         ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_29 (Add)                   (None, 16, 16, 32)   0           ['add_28[0][0]',                 \n",
      "                                                                  'batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 16, 16, 32)   9248        ['add_29[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 16, 16, 32)   0           ['conv2d_65[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_33[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 16, 16, 32)  128         ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_30 (Add)                   (None, 16, 16, 32)   0           ['add_29[0][0]',                 \n",
      "                                                                  'batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 16, 16, 32)   9248        ['add_30[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 16, 16, 32)   0           ['conv2d_67[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 16, 16, 32)  128         ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_31 (Add)                   (None, 16, 16, 32)   0           ['add_30[0][0]',                 \n",
      "                                                                  'batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 16, 16, 32)   9248        ['add_31[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 16, 16, 32)   0           ['conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 16, 16, 32)  128         ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_32 (Add)                   (None, 16, 16, 32)   0           ['add_31[0][0]',                 \n",
      "                                                                  'batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 16, 16, 32)   9248        ['add_32[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 16, 16, 32)   0           ['conv2d_71[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 16, 16, 32)  128         ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_33 (Add)                   (None, 16, 16, 32)   0           ['add_32[0][0]',                 \n",
      "                                                                  'batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 16, 16, 32)   9248        ['add_33[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 16, 16, 32)   0           ['conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 16, 16, 32)  128         ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_34 (Add)                   (None, 16, 16, 32)   0           ['add_33[0][0]',                 \n",
      "                                                                  'batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 8, 8, 64)     18496       ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 8, 8, 64)     0           ['conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 8, 8, 64)     2112        ['add_34[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_38[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 8, 8, 64)    256         ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 8, 8, 64)    256         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_35 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_41[0][0]', \n",
      "                                                                  'batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 8, 8, 64)     36928       ['add_35[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 8, 8, 64)     0           ['conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 8, 8, 64)    256         ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_36 (Add)                   (None, 8, 8, 64)     0           ['add_35[0][0]',                 \n",
      "                                                                  'batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 8, 8, 64)     36928       ['add_36[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 8, 8, 64)     0           ['conv2d_80[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_40[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 8, 8, 64)    256         ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_37 (Add)                   (None, 8, 8, 64)     0           ['add_36[0][0]',                 \n",
      "                                                                  'batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 8, 8, 64)     36928       ['add_37[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 8, 8, 64)     0           ['conv2d_82[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 8, 8, 64)    256         ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_38 (Add)                   (None, 8, 8, 64)     0           ['add_37[0][0]',                 \n",
      "                                                                  'batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 8, 8, 64)     36928       ['add_38[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 8, 8, 64)     0           ['conv2d_84[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 8, 8, 64)    256         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 8, 8, 64)     0           ['add_38[0][0]',                 \n",
      "                                                                  'batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 8, 8, 64)     36928       ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 8, 8, 64)     0           ['conv2d_86[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_43[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 8, 8, 64)    256         ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 8, 8, 64)     0           ['add_39[0][0]',                 \n",
      "                                                                  'batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 8, 8, 64)     36928       ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 8, 8, 64)     0           ['conv2d_88[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 8, 8, 64)    256         ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 8, 8, 64)     0           ['add_40[0][0]',                 \n",
      "                                                                  'batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 8, 8, 64)     0           ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 1, 1, 64)    0           ['activation_45[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 64)           0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 10)           650         ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 663,242\n",
      "Trainable params: 661,450\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:53f8urzj) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet-ReLU</strong> at: <a href='https://wandb.ai/hcim/CIFAR10/runs/53f8urzj' target=\"_blank\">https://wandb.ai/hcim/CIFAR10/runs/53f8urzj</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230414_095231-53f8urzj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:53f8urzj). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f683457ca34f7fb5301c5bd3052241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669742297381163, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/wandb/run-20230414_095503-cd12zi9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR10/runs/cd12zi9f' target=\"_blank\">ResNet-ReLU</a></strong> to <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR10' target=\"_blank\">https://wandb.ai/hcim/CIFAR10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR10/runs/cd12zi9f' target=\"_blank\">https://wandb.ai/hcim/CIFAR10/runs/cd12zi9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR10/runs/cd12zi9f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fee6b61bc10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR10\", entity=\"hcim\", name='ResNet-ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n",
      "391/391 [==============================] - 22s 51ms/step - loss: 2.6566 - accuracy: 0.0994 - val_loss: 2.6063 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.5560 - accuracy: 0.1199 - val_loss: 2.8555 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2930 - accuracy: 0.2439 - val_loss: 126.6675 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.2656 - accuracy: 0.2938 - val_loss: 6.2379 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.0675 - accuracy: 0.3598 - val_loss: 3.5254 - val_accuracy: 0.0999 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.9122 - accuracy: 0.4135 - val_loss: 22.8102 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7594 - accuracy: 0.4655 - val_loss: 2.7216 - val_accuracy: 0.2585 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.6077 - accuracy: 0.5191 - val_loss: 1.9795 - val_accuracy: 0.4773 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5026 - accuracy: 0.5584 - val_loss: 8.3608 - val_accuracy: 0.1269 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3978 - accuracy: 0.5995 - val_loss: 4.4839 - val_accuracy: 0.1051 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3215 - accuracy: 0.6235 - val_loss: 23.4264 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.2675 - accuracy: 0.6427 - val_loss: 1.9045 - val_accuracy: 0.4237 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2245 - accuracy: 0.6593 - val_loss: 1.9996 - val_accuracy: 0.4982 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1972 - accuracy: 0.6678 - val_loss: 7.2499 - val_accuracy: 0.1004 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.1670 - accuracy: 0.6797 - val_loss: 2.9368 - val_accuracy: 0.1906 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1284 - accuracy: 0.6936 - val_loss: 1.9401 - val_accuracy: 0.4985 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1025 - accuracy: 0.6991 - val_loss: 6.4539 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0845 - accuracy: 0.7098 - val_loss: 1.0049 - val_accuracy: 0.7442 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 20s 49ms/step - loss: 1.0531 - accuracy: 0.7196 - val_loss: 2.2091 - val_accuracy: 0.4107 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0857 - accuracy: 0.7111 - val_loss: 3.1588 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.0437 - accuracy: 0.7288 - val_loss: 0.9261 - val_accuracy: 0.7716 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0149 - accuracy: 0.7378 - val_loss: 1.1349 - val_accuracy: 0.7230 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0256 - accuracy: 0.7336 - val_loss: 3.2043 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9885 - accuracy: 0.7497 - val_loss: 16.0951 - val_accuracy: 0.1073 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9754 - accuracy: 0.7529 - val_loss: 2.6846 - val_accuracy: 0.2253 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 26/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9801 - accuracy: 0.7517 - val_loss: 2.7518 - val_accuracy: 0.1889 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 27/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9676 - accuracy: 0.7552 - val_loss: 36.5847 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 28/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9745 - accuracy: 0.7529 - val_loss: 3.5166 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 29/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9595 - accuracy: 0.7602 - val_loss: 1.8582 - val_accuracy: 0.4747 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 30/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9697 - accuracy: 0.7558 - val_loss: 2.2183 - val_accuracy: 0.4433 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9593 - accuracy: 0.7629 - val_loss: 4.0790 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9249 - accuracy: 0.7751 - val_loss: 1.2844 - val_accuracy: 0.6804 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9364 - accuracy: 0.7709 - val_loss: 4.1235 - val_accuracy: 0.1206 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9171 - accuracy: 0.7749 - val_loss: 2.2142 - val_accuracy: 0.3836 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 2.4462 - accuracy: 0.1896 - val_loss: 18.9790 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.6711 - accuracy: 0.4997 - val_loss: 3.2299 - val_accuracy: 0.1317 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.1768 - accuracy: 0.6874 - val_loss: 3.3429 - val_accuracy: 0.2318 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0732 - accuracy: 0.7293 - val_loss: 1.5478 - val_accuracy: 0.5777 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0428 - accuracy: 0.7406 - val_loss: 4.7137 - val_accuracy: 0.0997 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 40/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 1.0203 - accuracy: 0.7498 - val_loss: 1.1859 - val_accuracy: 0.7226 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 41/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9977 - accuracy: 0.7573 - val_loss: 0.8993 - val_accuracy: 0.7981 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 42/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9801 - accuracy: 0.7633 - val_loss: 1.8865 - val_accuracy: 0.4615 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 43/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9650 - accuracy: 0.7692 - val_loss: 0.9137 - val_accuracy: 0.7924 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 44/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9430 - accuracy: 0.7776 - val_loss: 0.9928 - val_accuracy: 0.7702 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9804 - accuracy: 0.7659 - val_loss: 3.8428 - val_accuracy: 0.1114 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9744 - accuracy: 0.7718 - val_loss: 3.1159 - val_accuracy: 0.3595 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.9580 - accuracy: 0.7748 - val_loss: 1.2848 - val_accuracy: 0.6823 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9504 - accuracy: 0.7784 - val_loss: 1.3607 - val_accuracy: 0.6570 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9417 - accuracy: 0.7810 - val_loss: 2.4399 - val_accuracy: 0.3692 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.9342 - accuracy: 0.7829 - val_loss: 1.0156 - val_accuracy: 0.7787 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9247 - accuracy: 0.7868 - val_loss: 2.2323 - val_accuracy: 0.4487 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8163 - accuracy: 0.8225 - val_loss: 0.6926 - val_accuracy: 0.8719 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7438 - accuracy: 0.8462 - val_loss: 0.6518 - val_accuracy: 0.8837 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.7184 - accuracy: 0.8543 - val_loss: 0.6449 - val_accuracy: 0.8851 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 55/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6983 - accuracy: 0.8605 - val_loss: 0.6415 - val_accuracy: 0.8816 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 56/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.6831 - accuracy: 0.8643 - val_loss: 0.6232 - val_accuracy: 0.8896 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 57/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6754 - accuracy: 0.8649 - val_loss: 0.6169 - val_accuracy: 0.8885 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 58/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6602 - accuracy: 0.8689 - val_loss: 0.6400 - val_accuracy: 0.8779 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 59/180\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.6538 - accuracy: 0.8699 - val_loss: 0.6108 - val_accuracy: 0.8878 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 60/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6420 - accuracy: 0.8719 - val_loss: 0.5980 - val_accuracy: 0.8916 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 61/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6334 - accuracy: 0.8739 - val_loss: 0.6303 - val_accuracy: 0.8823 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 62/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6254 - accuracy: 0.8748 - val_loss: 0.5919 - val_accuracy: 0.8915 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 63/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6187 - accuracy: 0.8767 - val_loss: 0.6736 - val_accuracy: 0.8630 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 64/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6108 - accuracy: 0.8787 - val_loss: 0.5741 - val_accuracy: 0.8936 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.5993 - accuracy: 0.8794 - val_loss: 0.6269 - val_accuracy: 0.8763 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5973 - accuracy: 0.8801 - val_loss: 0.5953 - val_accuracy: 0.8867 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5925 - accuracy: 0.8801 - val_loss: 0.6070 - val_accuracy: 0.8822 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5837 - accuracy: 0.8822 - val_loss: 0.6004 - val_accuracy: 0.8826 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5776 - accuracy: 0.8844 - val_loss: 0.5631 - val_accuracy: 0.8917 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5744 - accuracy: 0.8835 - val_loss: 0.5562 - val_accuracy: 0.8943 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.5709 - accuracy: 0.8835 - val_loss: 0.5953 - val_accuracy: 0.8801 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5589 - accuracy: 0.8876 - val_loss: 0.5623 - val_accuracy: 0.8911 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5521 - accuracy: 0.8874 - val_loss: 0.6332 - val_accuracy: 0.8716 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5499 - accuracy: 0.8873 - val_loss: 0.8287 - val_accuracy: 0.8174 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.5494 - accuracy: 0.8875 - val_loss: 0.5367 - val_accuracy: 0.8981 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5382 - accuracy: 0.8897 - val_loss: 0.5992 - val_accuracy: 0.8789 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5345 - accuracy: 0.8914 - val_loss: 0.5502 - val_accuracy: 0.8912 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.5298 - accuracy: 0.8910 - val_loss: 0.5360 - val_accuracy: 0.8967 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5254 - accuracy: 0.8913 - val_loss: 0.5680 - val_accuracy: 0.8859 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5287 - accuracy: 0.8905 - val_loss: 0.5786 - val_accuracy: 0.8833 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5158 - accuracy: 0.8944 - val_loss: 0.5485 - val_accuracy: 0.8909 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5182 - accuracy: 0.8918 - val_loss: 0.5809 - val_accuracy: 0.8805 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5098 - accuracy: 0.8939 - val_loss: 0.5579 - val_accuracy: 0.8911 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.5072 - accuracy: 0.8952 - val_loss: 0.5227 - val_accuracy: 0.8956 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5008 - accuracy: 0.8956 - val_loss: 0.5210 - val_accuracy: 0.8971 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4983 - accuracy: 0.8940 - val_loss: 0.5270 - val_accuracy: 0.8951 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4986 - accuracy: 0.8954 - val_loss: 0.5485 - val_accuracy: 0.8875 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4903 - accuracy: 0.8961 - val_loss: 0.5307 - val_accuracy: 0.8914 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 89/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4868 - accuracy: 0.8972 - val_loss: 0.6588 - val_accuracy: 0.8545 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 90/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.4880 - accuracy: 0.8963 - val_loss: 0.6567 - val_accuracy: 0.8523 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 91/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4850 - accuracy: 0.8974 - val_loss: 0.5189 - val_accuracy: 0.8963 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 92/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4790 - accuracy: 0.8977 - val_loss: 0.5212 - val_accuracy: 0.8965 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 93/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.4723 - accuracy: 0.8988 - val_loss: 0.6042 - val_accuracy: 0.8705 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4772 - accuracy: 0.8977 - val_loss: 0.6026 - val_accuracy: 0.8694 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4679 - accuracy: 0.8998 - val_loss: 0.6257 - val_accuracy: 0.8643 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4735 - accuracy: 0.8982 - val_loss: 0.5616 - val_accuracy: 0.8819 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4633 - accuracy: 0.9009 - val_loss: 0.5269 - val_accuracy: 0.8895 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4618 - accuracy: 0.9002 - val_loss: 0.5177 - val_accuracy: 0.8972 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.4600 - accuracy: 0.9001 - val_loss: 0.5056 - val_accuracy: 0.8962 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4611 - accuracy: 0.9001 - val_loss: 0.5992 - val_accuracy: 0.8694 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4513 - accuracy: 0.9037 - val_loss: 0.5036 - val_accuracy: 0.8951 - lr: 0.0100\n",
      "Learning rate: 0.001\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.4318 - accuracy: 0.9104 - val_loss: 0.4768 - val_accuracy: 0.9018 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4158 - accuracy: 0.9158 - val_loss: 0.4721 - val_accuracy: 0.9034 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4114 - accuracy: 0.9184 - val_loss: 0.4711 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 105/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4064 - accuracy: 0.9191 - val_loss: 0.4739 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 106/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.4078 - accuracy: 0.9187 - val_loss: 0.4730 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 107/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3991 - accuracy: 0.9223 - val_loss: 0.4726 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 108/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3962 - accuracy: 0.9220 - val_loss: 0.4734 - val_accuracy: 0.9052 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3984 - accuracy: 0.9213 - val_loss: 0.4695 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3950 - accuracy: 0.9224 - val_loss: 0.4712 - val_accuracy: 0.9052 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 111/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3961 - accuracy: 0.9217 - val_loss: 0.4679 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 112/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3943 - accuracy: 0.9230 - val_loss: 0.4707 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 113/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3929 - accuracy: 0.9229 - val_loss: 0.4668 - val_accuracy: 0.9060 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 114/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3894 - accuracy: 0.9251 - val_loss: 0.4714 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 115/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3885 - accuracy: 0.9252 - val_loss: 0.4697 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3936 - accuracy: 0.9224 - val_loss: 0.4640 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3890 - accuracy: 0.9243 - val_loss: 0.4679 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 118/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3880 - accuracy: 0.9243 - val_loss: 0.4654 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 119/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3873 - accuracy: 0.9241 - val_loss: 0.4671 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 120/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3849 - accuracy: 0.9248 - val_loss: 0.4700 - val_accuracy: 0.9058 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 121/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3826 - accuracy: 0.9265 - val_loss: 0.4686 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 122/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3783 - accuracy: 0.9264 - val_loss: 0.4673 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3820 - accuracy: 0.9257 - val_loss: 0.4667 - val_accuracy: 0.9062 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3816 - accuracy: 0.9255 - val_loss: 0.4672 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3843 - accuracy: 0.9257 - val_loss: 0.4690 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3783 - accuracy: 0.9275 - val_loss: 0.4649 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3792 - accuracy: 0.9268 - val_loss: 0.4676 - val_accuracy: 0.9054 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3795 - accuracy: 0.9272 - val_loss: 0.4655 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3776 - accuracy: 0.9274 - val_loss: 0.4666 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 130/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.3746 - accuracy: 0.9279 - val_loss: 0.4673 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 131/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3713 - accuracy: 0.9284 - val_loss: 0.4677 - val_accuracy: 0.9056 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 132/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3752 - accuracy: 0.9274 - val_loss: 0.4705 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 133/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3735 - accuracy: 0.9276 - val_loss: 0.4680 - val_accuracy: 0.9058 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 134/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3755 - accuracy: 0.9268 - val_loss: 0.4666 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 135/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3731 - accuracy: 0.9270 - val_loss: 0.4752 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 136/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3709 - accuracy: 0.9277 - val_loss: 0.4655 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 137/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3724 - accuracy: 0.9292 - val_loss: 0.4679 - val_accuracy: 0.9074 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 138/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3711 - accuracy: 0.9293 - val_loss: 0.4729 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 139/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3711 - accuracy: 0.9273 - val_loss: 0.4681 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 140/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.3681 - accuracy: 0.9299 - val_loss: 0.4698 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 141/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3606 - accuracy: 0.9317 - val_loss: 0.4682 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 142/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3687 - accuracy: 0.9288 - val_loss: 0.4671 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 143/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3666 - accuracy: 0.9308 - val_loss: 0.4733 - val_accuracy: 0.9040 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 144/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3680 - accuracy: 0.9295 - val_loss: 0.4891 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 145/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3650 - accuracy: 0.9302 - val_loss: 0.4687 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 146/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3707 - accuracy: 0.9277 - val_loss: 0.4651 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 147/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3637 - accuracy: 0.9302 - val_loss: 0.4694 - val_accuracy: 0.9060 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 148/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3626 - accuracy: 0.9306 - val_loss: 0.4674 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 149/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3641 - accuracy: 0.9306 - val_loss: 0.4671 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 150/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3581 - accuracy: 0.9319 - val_loss: 0.4678 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 151/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3613 - accuracy: 0.9311 - val_loss: 0.4684 - val_accuracy: 0.9060 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 152/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3638 - accuracy: 0.9292 - val_loss: 0.4723 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 153/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3602 - accuracy: 0.9316 - val_loss: 0.4681 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 154/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3599 - accuracy: 0.9319 - val_loss: 0.4743 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 155/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.3629 - accuracy: 0.9298 - val_loss: 0.4677 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 156/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3583 - accuracy: 0.9316 - val_loss: 0.4655 - val_accuracy: 0.9078 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 157/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3584 - accuracy: 0.9314 - val_loss: 0.4640 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 158/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3620 - accuracy: 0.9286 - val_loss: 0.4668 - val_accuracy: 0.9048 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 159/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3567 - accuracy: 0.9306 - val_loss: 0.4666 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 160/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3582 - accuracy: 0.9303 - val_loss: 0.4639 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 161/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3529 - accuracy: 0.9330 - val_loss: 0.4688 - val_accuracy: 0.9052 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3511 - accuracy: 0.9331 - val_loss: 0.4667 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3532 - accuracy: 0.9335 - val_loss: 0.4664 - val_accuracy: 0.9076 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3521 - accuracy: 0.9338 - val_loss: 0.4686 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3518 - accuracy: 0.9332 - val_loss: 0.4675 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3521 - accuracy: 0.9337 - val_loss: 0.4699 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3541 - accuracy: 0.9315 - val_loss: 0.4667 - val_accuracy: 0.9068 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3530 - accuracy: 0.9331 - val_loss: 0.4645 - val_accuracy: 0.9064 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 169/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3551 - accuracy: 0.9310 - val_loss: 0.4657 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 170/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3536 - accuracy: 0.9316 - val_loss: 0.4654 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 171/180\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.3497 - accuracy: 0.9331 - val_loss: 0.4670 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 172/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3473 - accuracy: 0.9339 - val_loss: 0.4660 - val_accuracy: 0.9070 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 173/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3488 - accuracy: 0.9330 - val_loss: 0.4656 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 174/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3427 - accuracy: 0.9357 - val_loss: 0.4714 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 175/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3483 - accuracy: 0.9339 - val_loss: 0.4701 - val_accuracy: 0.9058 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 176/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3474 - accuracy: 0.9324 - val_loss: 0.4717 - val_accuracy: 0.9062 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 177/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3493 - accuracy: 0.9338 - val_loss: 0.4627 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 178/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3433 - accuracy: 0.9336 - val_loss: 0.4670 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 179/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3476 - accuracy: 0.9322 - val_loss: 0.4620 - val_accuracy: 0.9062 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.3394 - accuracy: 0.9368 - val_loss: 0.4733 - val_accuracy: 0.9058 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff00913f520>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e946-b5c5-48d8-90e6-1144be24915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
