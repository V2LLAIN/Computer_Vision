{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=100)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=100):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "        network = Activation('relu')(network)\n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              strides=1, activation='relu', normalize_batch=True):\n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 100:\n",
    "        pass\n",
    "    elif epoch > 100 and epoch <= 150:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 32, 32, 16)   448         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 32, 32, 16)  64          ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 32, 32, 16)   0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 32, 32, 16)   0           ['conv2d_91[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 32, 32, 16)  64          ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 32, 32, 16)   0           ['activation_46[0][0]',          \n",
      "                                                                  'batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 32, 32, 16)   2320        ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 32, 32, 16)   0           ['conv2d_93[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_48[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 32, 32, 16)  64          ['conv2d_94[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_43 (Add)                   (None, 32, 32, 16)   0           ['add_42[0][0]',                 \n",
      "                                                                  'batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)             (None, 32, 32, 16)   2320        ['add_43[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 32, 32, 16)   0           ['conv2d_95[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 32, 32, 16)  64          ['conv2d_96[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_44 (Add)                   (None, 32, 32, 16)   0           ['add_43[0][0]',                 \n",
      "                                                                  'batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)             (None, 32, 32, 16)   2320        ['add_44[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 32, 32, 16)   0           ['conv2d_97[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_50[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 32, 32, 16)  64          ['conv2d_98[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_45 (Add)                   (None, 32, 32, 16)   0           ['add_44[0][0]',                 \n",
      "                                                                  'batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)             (None, 32, 32, 16)   2320        ['add_45[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 32, 32, 16)   0           ['conv2d_99[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)            (None, 32, 32, 16)   2320        ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 32, 32, 16)  64          ['conv2d_100[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_46 (Add)                   (None, 32, 32, 16)   0           ['add_45[0][0]',                 \n",
      "                                                                  'batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)            (None, 32, 32, 16)   2320        ['add_46[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 32, 32, 16)   0           ['conv2d_101[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)            (None, 32, 32, 16)   2320        ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 32, 32, 16)  64          ['conv2d_102[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 32, 32, 16)   0           ['add_46[0][0]',                 \n",
      "                                                                  'batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)            (None, 32, 32, 16)   2320        ['add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 32, 32, 16)   0           ['conv2d_103[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)            (None, 32, 32, 16)   2320        ['activation_53[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 32, 32, 16)  64          ['conv2d_104[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 32, 32, 16)   0           ['add_47[0][0]',                 \n",
      "                                                                  'batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)            (None, 16, 16, 32)   4640        ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 16, 16, 32)   0           ['conv2d_105[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 16, 16, 32)   544         ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 16, 16, 32)  128         ['conv2d_107[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 16, 16, 32)  128         ['conv2d_106[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 16, 16, 32)   0           ['batch_normalization_57[0][0]', \n",
      "                                                                  'batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 16, 16, 32)   9248        ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 16, 16, 32)   0           ['conv2d_108[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 16, 16, 32)  128         ['conv2d_109[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 16, 16, 32)   0           ['add_49[0][0]',                 \n",
      "                                                                  'batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 16, 16, 32)   9248        ['add_50[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 16, 16, 32)   0           ['conv2d_110[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 16, 16, 32)  128         ['conv2d_111[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 16, 16, 32)   0           ['add_50[0][0]',                 \n",
      "                                                                  'batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 16, 16, 32)   9248        ['add_51[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 16, 16, 32)   0           ['conv2d_112[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 16, 16, 32)  128         ['conv2d_113[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 16, 16, 32)   0           ['add_51[0][0]',                 \n",
      "                                                                  'batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 16, 16, 32)   9248        ['add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 16, 16, 32)   0           ['conv2d_114[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_58[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 16, 16, 32)  128         ['conv2d_115[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 16, 16, 32)   0           ['add_52[0][0]',                 \n",
      "                                                                  'batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)            (None, 16, 16, 32)   9248        ['add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 16, 16, 32)   0           ['conv2d_116[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_117 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 16, 16, 32)  128         ['conv2d_117[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 16, 16, 32)   0           ['add_53[0][0]',                 \n",
      "                                                                  'batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_118 (Conv2D)            (None, 16, 16, 32)   9248        ['add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 16, 16, 32)   0           ['conv2d_118[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_119 (Conv2D)            (None, 16, 16, 32)   9248        ['activation_60[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 16, 16, 32)  128         ['conv2d_119[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 16, 16, 32)   0           ['add_54[0][0]',                 \n",
      "                                                                  'batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_120 (Conv2D)            (None, 8, 8, 64)     18496       ['add_55[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 8, 8, 64)     0           ['conv2d_120[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_122 (Conv2D)            (None, 8, 8, 64)     2112        ['add_55[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_121 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 8, 8, 64)    256         ['conv2d_122[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 8, 8, 64)    256         ['conv2d_121[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_65[0][0]', \n",
      "                                                                  'batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_123 (Conv2D)            (None, 8, 8, 64)     36928       ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 8, 8, 64)     0           ['conv2d_123[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_124 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 8, 8, 64)    256         ['conv2d_124[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 8, 8, 64)     0           ['add_56[0][0]',                 \n",
      "                                                                  'batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_125 (Conv2D)            (None, 8, 8, 64)     36928       ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 8, 8, 64)     0           ['conv2d_125[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_126 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_63[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 8, 8, 64)    256         ['conv2d_126[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 8, 8, 64)     0           ['add_57[0][0]',                 \n",
      "                                                                  'batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_127 (Conv2D)            (None, 8, 8, 64)     36928       ['add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 8, 8, 64)     0           ['conv2d_127[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_128 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_64[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 8, 8, 64)    256         ['conv2d_128[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 8, 8, 64)     0           ['add_58[0][0]',                 \n",
      "                                                                  'batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_129 (Conv2D)            (None, 8, 8, 64)     36928       ['add_59[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 8, 8, 64)     0           ['conv2d_129[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_130 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 8, 8, 64)    256         ['conv2d_130[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_60 (Add)                   (None, 8, 8, 64)     0           ['add_59[0][0]',                 \n",
      "                                                                  'batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_131 (Conv2D)            (None, 8, 8, 64)     36928       ['add_60[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 8, 8, 64)     0           ['conv2d_131[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_132 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 8, 8, 64)    256         ['conv2d_132[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_61 (Add)                   (None, 8, 8, 64)     0           ['add_60[0][0]',                 \n",
      "                                                                  'batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_133 (Conv2D)            (None, 8, 8, 64)     36928       ['add_61[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 8, 8, 64)     0           ['conv2d_133[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_134 (Conv2D)            (None, 8, 8, 64)     36928       ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 8, 8, 64)    256         ['conv2d_134[0][0]']             \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_62 (Add)                   (None, 8, 8, 64)     0           ['add_61[0][0]',                 \n",
      "                                                                  'batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 8, 8, 64)     0           ['add_62[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 1, 1, 64)    0           ['activation_68[0][0]']          \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 64)           0           ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          6500        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 669,092\n",
      "Trainable params: 667,300\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:cmdc8s0a) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574ddfbf018e4696b37b320760b66a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.071 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.029086…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▇▂▇▆▄▆▃▇▅▃▇▅▄▇▅▅▃▆▅█▄▄▆▃▄▄▃▅▅▅▃▅▄▃▇▄▆▁▄▆</td></tr><tr><td>loss</td><td>█▇▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.00958</td></tr><tr><td>loss</td><td>4.60832</td></tr><tr><td>val_accuracy</td><td>0.01</td></tr><tr><td>val_loss</td><td>4.60675</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ResNet-ReLU</strong> at: <a href='https://wandb.ai/hcim/CIFAR100/runs/cmdc8s0a' target=\"_blank\">https://wandb.ai/hcim/CIFAR100/runs/cmdc8s0a</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230416_103505-cmdc8s0a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:cmdc8s0a). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578d28839f4848dbbe2a071e18a3f836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666970216513922, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/wandb/run-20230416_105845-ct8h7ebe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR100/runs/ct8h7ebe' target=\"_blank\">ResNet-ReLU</a></strong> to <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">https://wandb.ai/hcim/CIFAR100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR100/runs/ct8h7ebe' target=\"_blank\">https://wandb.ai/hcim/CIFAR100/runs/ct8h7ebe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR100/runs/ct8h7ebe?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb86d66ba90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR100\", entity=\"hcim\", name='ResNet-ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n",
      "391/391 [==============================] - 22s 50ms/step - loss: 4.9476 - accuracy: 0.0092 - val_loss: 4.9008 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.8808 - accuracy: 0.0096 - val_loss: 4.8583 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.8415 - accuracy: 0.0091 - val_loss: 4.8217 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.8076 - accuracy: 0.0085 - val_loss: 4.7905 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.7787 - accuracy: 0.0104 - val_loss: 4.7637 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.7539 - accuracy: 0.0090 - val_loss: 4.7408 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.7328 - accuracy: 0.0098 - val_loss: 4.7216 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.7145 - accuracy: 0.0090 - val_loss: 4.7050 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6992 - accuracy: 0.0088 - val_loss: 4.6904 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6860 - accuracy: 0.0095 - val_loss: 4.6786 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6747 - accuracy: 0.0087 - val_loss: 4.6681 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6651 - accuracy: 0.0091 - val_loss: 4.6589 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6567 - accuracy: 0.0093 - val_loss: 4.6516 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6498 - accuracy: 0.0084 - val_loss: 4.6446 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 19s 50ms/step - loss: 4.6440 - accuracy: 0.0092 - val_loss: 4.6392 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6384 - accuracy: 0.0092 - val_loss: 4.6345 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6342 - accuracy: 0.0094 - val_loss: 4.6305 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6306 - accuracy: 0.0093 - val_loss: 4.6268 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6272 - accuracy: 0.0087 - val_loss: 4.6239 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6244 - accuracy: 0.0103 - val_loss: 4.6217 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6220 - accuracy: 0.0096 - val_loss: 4.6194 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6201 - accuracy: 0.0097 - val_loss: 4.6174 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6188 - accuracy: 0.0093 - val_loss: 4.6160 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6171 - accuracy: 0.0094 - val_loss: 4.6146 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6158 - accuracy: 0.0091 - val_loss: 4.6136 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 26/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6146 - accuracy: 0.0093 - val_loss: 4.6122 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 27/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6137 - accuracy: 0.0097 - val_loss: 4.6115 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 28/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6127 - accuracy: 0.0092 - val_loss: 4.6106 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 29/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6123 - accuracy: 0.0093 - val_loss: 4.6099 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 30/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6117 - accuracy: 0.0096 - val_loss: 4.6098 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6132 - accuracy: 0.0097 - val_loss: 4.6109 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6125 - accuracy: 0.0094 - val_loss: 4.6100 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6113 - accuracy: 0.0090 - val_loss: 4.6100 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6112 - accuracy: 0.0085 - val_loss: 4.6090 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6105 - accuracy: 0.0090 - val_loss: 4.6086 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6100 - accuracy: 0.0097 - val_loss: 4.6081 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6098 - accuracy: 0.0094 - val_loss: 4.6079 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6096 - accuracy: 0.0092 - val_loss: 4.6077 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6095 - accuracy: 0.0087 - val_loss: 4.6073 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 40/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6091 - accuracy: 0.0093 - val_loss: 4.6072 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 41/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6092 - accuracy: 0.0097 - val_loss: 4.6074 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 42/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6095 - accuracy: 0.0094 - val_loss: 4.6076 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 43/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6091 - accuracy: 0.0092 - val_loss: 4.6076 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 44/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6091 - accuracy: 0.0096 - val_loss: 4.6071 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6089 - accuracy: 0.0087 - val_loss: 4.6072 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6089 - accuracy: 0.0090 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6084 - accuracy: 0.0094 - val_loss: 4.6069 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6085 - accuracy: 0.0084 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6084 - accuracy: 0.0092 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6082 - accuracy: 0.0097 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 4.6083 - accuracy: 0.0093 - val_loss: 4.6064 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6094 - accuracy: 0.0086 - val_loss: 4.6078 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6093 - accuracy: 0.0090 - val_loss: 4.6074 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6091 - accuracy: 0.0097 - val_loss: 4.6073 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 55/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6089 - accuracy: 0.0091 - val_loss: 4.6072 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 56/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6087 - accuracy: 0.0085 - val_loss: 4.6071 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 57/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6088 - accuracy: 0.0097 - val_loss: 4.6069 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 58/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6086 - accuracy: 0.0089 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 59/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6085 - accuracy: 0.0094 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 60/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6082 - accuracy: 0.0092 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 61/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6084 - accuracy: 0.0092 - val_loss: 4.6063 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 62/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6082 - accuracy: 0.0091 - val_loss: 4.6066 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 63/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6089 - accuracy: 0.0093 - val_loss: 4.6072 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 64/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6087 - accuracy: 0.0091 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6084 - accuracy: 0.0102 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6084 - accuracy: 0.0099 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6082 - accuracy: 0.0093 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6087 - accuracy: 0.0094 - val_loss: 4.6066 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6085 - accuracy: 0.0090 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6084 - accuracy: 0.0091 - val_loss: 4.6065 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6084 - accuracy: 0.0088 - val_loss: 4.6066 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6082 - accuracy: 0.0089 - val_loss: 4.6065 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6181 - accuracy: 0.0102 - val_loss: 4.6161 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6171 - accuracy: 0.0092 - val_loss: 4.6144 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6156 - accuracy: 0.0099 - val_loss: 4.6134 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 4.6145 - accuracy: 0.0091 - val_loss: 4.6123 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6135 - accuracy: 0.0096 - val_loss: 4.6115 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6128 - accuracy: 0.0088 - val_loss: 4.6107 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6120 - accuracy: 0.0093 - val_loss: 4.6101 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6114 - accuracy: 0.0093 - val_loss: 4.6092 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6108 - accuracy: 0.0095 - val_loss: 4.6090 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6105 - accuracy: 0.0093 - val_loss: 4.6086 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6100 - accuracy: 0.0102 - val_loss: 4.6083 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6100 - accuracy: 0.0098 - val_loss: 4.6078 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6095 - accuracy: 0.0093 - val_loss: 4.6077 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6092 - accuracy: 0.0091 - val_loss: 4.6073 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6093 - accuracy: 0.0086 - val_loss: 4.6071 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6092 - accuracy: 0.0096 - val_loss: 4.6073 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 89/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6089 - accuracy: 0.0097 - val_loss: 4.6070 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 90/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6089 - accuracy: 0.0090 - val_loss: 4.6069 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 91/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6086 - accuracy: 0.0093 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 92/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6083 - accuracy: 0.0094 - val_loss: 4.6068 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 93/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6083 - accuracy: 0.0087 - val_loss: 4.6067 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6083 - accuracy: 0.0091 - val_loss: 4.6065 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6083 - accuracy: 0.0091 - val_loss: 4.6066 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 4.6081 - accuracy: 0.0088 - val_loss: 4.6064 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6082 - accuracy: 0.0090 - val_loss: 4.6062 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6082 - accuracy: 0.0086 - val_loss: 4.6062 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6080 - accuracy: 0.0091 - val_loss: 4.6063 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6081 - accuracy: 0.0097 - val_loss: 4.6063 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6080 - accuracy: 0.0096 - val_loss: 4.6062 - val_accuracy: 0.0100 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6064 - accuracy: 0.0100 - val_loss: 4.6058 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6060 - accuracy: 0.0100 - val_loss: 4.6055 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6058 - accuracy: 0.0092 - val_loss: 4.6054 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 105/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6058 - accuracy: 0.0094 - val_loss: 4.6054 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 106/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0092 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 107/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0094 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 108/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6057 - accuracy: 0.0085 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0084 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0086 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0090 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6057 - accuracy: 0.0085 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 118/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0093 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 119/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0087 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 120/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0087 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 121/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6056 - accuracy: 0.0088 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 122/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0091 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0091 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6056 - accuracy: 0.0085 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0084 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0087 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6056 - accuracy: 0.0090 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0091 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6056 - accuracy: 0.0081 - val_loss: 4.6053 - val_accuracy: 0.0100 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 130/180\n",
      "223/391 [================>.............] - ETA: 7s - loss: 4.6055 - accuracy: 0.0091"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6053 - accuracy: 0.0092 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6053 - accuracy: 0.0093 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0096 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0098 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6053 - accuracy: 0.0089 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0089 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0097 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0090 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 175/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6053 - accuracy: 0.0086 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 176/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0089 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 177/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0095 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 178/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 4.6053 - accuracy: 0.0100 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 179/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0098 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 4.6053 - accuracy: 0.0085 - val_loss: 4.6052 - val_accuracy: 0.0100 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb86d6f57f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e946-b5c5-48d8-90e6-1144be24915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
