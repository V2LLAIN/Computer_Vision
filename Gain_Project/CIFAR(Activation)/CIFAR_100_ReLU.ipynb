{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435f2fe4-a4ef-4b66-a91f-6b6c598baa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-04-15 12:47:08.013503: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 12:47:08.093746: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-15 12:47:08.115441: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.metrics import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.activations import *\n",
    "\n",
    "from tensorflow.keras.regularizers import *\n",
    "\n",
    "from tensorflow.keras.callbacks import *\n",
    "from keras.preprocessing.image import *\n",
    "from tensorflow.keras.preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6970ba03-d344-4b3a-aca9-5b6fd275830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=100)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882e3131-fea4-4ec8-ae35-5c508266c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 180\n",
    "n_classes = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2dac364-264e-4823-8695-e36d850d7ebe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Resnet:\n",
    "    def __init__(self, size=44, stacks=3, starting_filter=16):\n",
    "        self.size = size\n",
    "        self.stacks = stacks\n",
    "        self.starting_filter = starting_filter\n",
    "        self.residual_blocks = (size - 2) // 6\n",
    "        \n",
    "    def get_model(self, input_shape=(32, 32, 3), n_classes=100):\n",
    "        n_filters = self.starting_filter\n",
    "\n",
    "        inputs = Input(shape=input_shape)\n",
    "        network = self.layer(inputs, n_filters)\n",
    "        network = self.stack(network, n_filters, True)\n",
    "\n",
    "        for _ in range(self.stacks - 1):\n",
    "            n_filters *= 2\n",
    "            network = self.stack(network, n_filters)\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        network = Activation('relu')(network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        network = AveragePooling2D(pool_size=network.shape[1])(network)\n",
    "        network = Flatten()(network)\n",
    "        outputs = Dense(n_classes, activation='softmax', \n",
    "                        kernel_initializer='he_normal')(network)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def stack(self, inputs, n_filters, first_stack=False):\n",
    "        stack = inputs\n",
    "\n",
    "        if first_stack:\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "        else:\n",
    "            stack = self.convolution_block(stack, n_filters)\n",
    "\n",
    "        for _ in range(self.residual_blocks - 1):\n",
    "            stack = self.identity_block(stack, n_filters)\n",
    "\n",
    "        return stack\n",
    "    \n",
    "    def identity_block(self, inputs, n_filters):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "\n",
    "    def convolution_block(self, inputs, n_filters, strides=2):\n",
    "        shortcut = inputs\n",
    "\n",
    "        block = self.layer(inputs, n_filters, strides=strides,\n",
    "                           normalize_batch=False)\n",
    "        block = self.layer(block, n_filters, activation=None)\n",
    "\n",
    "        shortcut = self.layer(shortcut, n_filters,\n",
    "                              kernel_size=1, strides=strides,\n",
    "                              activation=None)\n",
    "\n",
    "        block = Add()([shortcut, block])\n",
    "\n",
    "        return block\n",
    "    \n",
    "    def layer(self, inputs, n_filters, kernel_size=3,\n",
    "              \n",
    "              \n",
    "              \n",
    "              \n",
    "              strides=1, activation='relu', normalize_batch=True):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        convolution = Conv2D(n_filters, kernel_size=kernel_size,\n",
    "                             strides=strides, padding='same',\n",
    "                             kernel_initializer=\"he_normal\",\n",
    "                             kernel_regularizer=l2(1e-4))\n",
    "\n",
    "        x = convolution(inputs)\n",
    "\n",
    "        if normalize_batch:\n",
    "            x = BatchNormalization()(x)\n",
    "\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def learning_rate_schedule(epoch):\n",
    "    new_learning_rate = learning_rate\n",
    "\n",
    "    if epoch <= 50:\n",
    "        pass\n",
    "    elif epoch > 50 and epoch <= 100:\n",
    "        new_learning_rate = learning_rate * 0.1\n",
    "    else:\n",
    "        new_learning_rate = learning_rate * 0.01\n",
    "        \n",
    "    print('Learning rate:', new_learning_rate)\n",
    "    \n",
    "    return new_learning_rate\n",
    "\n",
    "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
    "    def eraser(input_img):\n",
    "        if input_img.ndim == 3:\n",
    "            img_h, img_w, img_c = input_img.shape\n",
    "        elif input_img.ndim == 2:\n",
    "            img_h, img_w = input_img.shape\n",
    "\n",
    "        p_1 = np.random.rand()\n",
    "\n",
    "        if p_1 > p:\n",
    "            return input_img\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, img_w)\n",
    "            top = np.random.randint(0, img_h)\n",
    "\n",
    "            if left + w <= img_w and top + h <= img_h:\n",
    "                break\n",
    "\n",
    "        if pixel_level:\n",
    "            if input_img.ndim == 3:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
    "            if input_img.ndim == 2:\n",
    "                c = np.random.uniform(v_l, v_h, (h, w))\n",
    "        else:\n",
    "            c = np.random.uniform(v_l, v_h)\n",
    "\n",
    "        input_img[top:top + h, left:left + w] = c\n",
    "\n",
    "        return input_img\n",
    "\n",
    "    return eraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9897576-4328-4349-90b8-2ac97f68f424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 12:47:09.946905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:09.948761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:09.948846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:09.949252: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-15 12:47:09.949623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:09.949734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:09.949803: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:10.298908: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:10.299045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:10.299124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-04-15 12:47:10.299197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['add[0][0]',                    \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['add_1[0][0]',                  \n",
      "                                                                  'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 32, 16)   2320        ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 32, 32, 16)   0           ['add_2[0][0]',                  \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 16)   2320        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 32, 32, 16)   0           ['add_3[0][0]',                  \n",
      "                                                                  'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 32, 32, 16)   2320        ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 32, 32, 16)   0           ['add_4[0][0]',                  \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 32, 32, 16)   2320        ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 32, 32, 16)   0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 32, 32, 16)   2320        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 32, 32, 16)   0           ['add_5[0][0]',                  \n",
      "                                                                  'batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 16, 16, 32)   4640        ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 16, 16, 32)   544         ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 16, 16, 32)   9248        ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 16, 16, 32)   0           ['add_7[0][0]',                  \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 16, 16, 32)   9248        ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 16, 16, 32)   0           ['add_8[0][0]',                  \n",
      "                                                                  'batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 16, 16, 32)   9248        ['add_9[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, 16, 16, 32)   0           ['add_9[0][0]',                  \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 16, 16, 32)   9248        ['add_10[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 16, 16, 32)  128         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, 16, 16, 32)   0           ['add_10[0][0]',                 \n",
      "                                                                  'batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 16, 16, 32)   9248        ['add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 16, 16, 32)   0           ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 16, 16, 32)  128         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 16, 16, 32)   0           ['add_11[0][0]',                 \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 16, 16, 32)   9248        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 16, 16, 32)   0           ['conv2d_28[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 16, 16, 32)  128         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 16, 16, 32)   0           ['add_12[0][0]',                 \n",
      "                                                                  'batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 8, 8, 64)     18496       ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['conv2d_30[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 8, 8, 64)     2112        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]', \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 8, 8, 64)     36928       ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['conv2d_33[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 8, 8, 64)     0           ['add_14[0][0]',                 \n",
      "                                                                  'batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 8, 8, 64)     36928       ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['conv2d_35[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 8, 8, 64)    256         ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, 8, 8, 64)     0           ['add_15[0][0]',                 \n",
      "                                                                  'batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 8, 8, 64)     36928       ['add_16[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['conv2d_37[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 8, 8, 64)    256         ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, 8, 8, 64)     0           ['add_16[0][0]',                 \n",
      "                                                                  'batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 8, 8, 64)     36928       ['add_17[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 8, 8, 64)     0           ['conv2d_39[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_19[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 8, 8, 64)    256         ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, 8, 8, 64)     0           ['add_17[0][0]',                 \n",
      "                                                                  'batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 8, 8, 64)     36928       ['add_18[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 8, 8, 64)     0           ['conv2d_41[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 8, 8, 64)    256         ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, 8, 8, 64)     0           ['add_18[0][0]',                 \n",
      "                                                                  'batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 8, 8, 64)     36928       ['add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 8, 8, 64)     0           ['conv2d_43[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_21[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 8, 8, 64)    256         ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 8, 8, 64)     0           ['add_19[0][0]',                 \n",
      "                                                                  'batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 8, 8, 64)     0           ['add_20[0][0]']                 \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_22[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 100)          6500        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 669,092\n",
      "Trainable params: 667,300\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet = Resnet()\n",
    "\n",
    "model = resnet.get_model()\n",
    "\n",
    "optimizer = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492f7e1e-df54-4686-aaca-ba346e556926",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "class WandbCallback(callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42566652-334a-4435-a44c-e74a79105484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/jupyter/IHC/wandb/run-20230415_124712-kxfzb1ti</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/CIFAR100/runs/kxfzb1ti' target=\"_blank\">ResNet-ReLU</a></strong> to <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/CIFAR100' target=\"_blank\">https://wandb.ai/hcim/CIFAR100</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/CIFAR100/runs/kxfzb1ti' target=\"_blank\">https://wandb.ai/hcim/CIFAR100/runs/kxfzb1ti</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hcim/CIFAR100/runs/kxfzb1ti?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f991f687100>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR100\", entity=\"hcim\", name='ResNet-ReLU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e995314-6225-4052-95d8-03e5235ea600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-15 12:47:16.300825: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2023-04-15 12:47:17.414754: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 23s 50ms/step - loss: 2.6263 - accuracy: 0.1385 - val_loss: 2.9212 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 2/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.3227 - accuracy: 0.2378 - val_loss: 3.8707 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 3/180\n",
      "391/391 [==============================] - 19s 48ms/step - loss: 2.1799 - accuracy: 0.2849 - val_loss: 3.0903 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 4/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 2.1061 - accuracy: 0.3089 - val_loss: 6.3803 - val_accuracy: 0.0988 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 5/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.8790 - accuracy: 0.3967 - val_loss: 70.2040 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 6/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.7669 - accuracy: 0.4443 - val_loss: 4.8431 - val_accuracy: 0.1002 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 7/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.5421 - accuracy: 0.5255 - val_loss: 2.0761 - val_accuracy: 0.4242 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 8/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.5005 - accuracy: 0.5436 - val_loss: 7.8560 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 9/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3456 - accuracy: 0.6007 - val_loss: 3.8335 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 10/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.3004 - accuracy: 0.6204 - val_loss: 54.2061 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 11/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2732 - accuracy: 0.6271 - val_loss: 3.1757 - val_accuracy: 0.1321 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 12/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.2117 - accuracy: 0.6525 - val_loss: 3.1642 - val_accuracy: 0.1985 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 13/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.1615 - accuracy: 0.6711 - val_loss: 3.1456 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 14/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0886 - accuracy: 0.6990 - val_loss: 1.9871 - val_accuracy: 0.4145 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 15/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0571 - accuracy: 0.7104 - val_loss: 1.1153 - val_accuracy: 0.6949 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 16/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0272 - accuracy: 0.7195 - val_loss: 1.5051 - val_accuracy: 0.5931 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 17/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0732 - accuracy: 0.7059 - val_loss: 2.0677 - val_accuracy: 0.4033 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 18/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0272 - accuracy: 0.7236 - val_loss: 3.7398 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 19/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9822 - accuracy: 0.7409 - val_loss: 3.9812 - val_accuracy: 0.1346 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 20/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 1.0256 - accuracy: 0.7255 - val_loss: 1.1315 - val_accuracy: 0.7032 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 21/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0088 - accuracy: 0.7360 - val_loss: 3.2329 - val_accuracy: 0.1567 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 22/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9882 - accuracy: 0.7424 - val_loss: 2.9525 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 23/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9528 - accuracy: 0.7555 - val_loss: 3.9708 - val_accuracy: 0.1176 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 24/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9719 - accuracy: 0.7504 - val_loss: 13.8408 - val_accuracy: 0.0999 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 25/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9769 - accuracy: 0.7502 - val_loss: 0.9585 - val_accuracy: 0.7700 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 26/180\n",
      "391/391 [==============================] - 19s 49ms/step - loss: 0.9497 - accuracy: 0.7608 - val_loss: 3.3917 - val_accuracy: 0.1076 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 27/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9118 - accuracy: 0.7743 - val_loss: 1.5261 - val_accuracy: 0.6022 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 28/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9129 - accuracy: 0.7735 - val_loss: 3.2508 - val_accuracy: 0.1224 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 29/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9104 - accuracy: 0.7734 - val_loss: 0.8756 - val_accuracy: 0.7880 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 30/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9010 - accuracy: 0.7773 - val_loss: 3.7340 - val_accuracy: 0.1850 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 31/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8827 - accuracy: 0.7845 - val_loss: 0.9601 - val_accuracy: 0.7717 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 32/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8869 - accuracy: 0.7826 - val_loss: 1.4755 - val_accuracy: 0.5864 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 33/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 1.0218 - accuracy: 0.7418 - val_loss: 0.8960 - val_accuracy: 0.7979 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 34/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9401 - accuracy: 0.7719 - val_loss: 3.3595 - val_accuracy: 0.1295 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 35/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9233 - accuracy: 0.7776 - val_loss: 1.6178 - val_accuracy: 0.5669 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 36/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9071 - accuracy: 0.7824 - val_loss: 1.6563 - val_accuracy: 0.5742 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 37/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8896 - accuracy: 0.7876 - val_loss: 1.1850 - val_accuracy: 0.7017 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 38/180\n",
      "391/391 [==============================] - 20s 49ms/step - loss: 0.8661 - accuracy: 0.7962 - val_loss: 1.3373 - val_accuracy: 0.6492 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 39/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8703 - accuracy: 0.7941 - val_loss: 1.0926 - val_accuracy: 0.7231 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 40/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8735 - accuracy: 0.7927 - val_loss: 2.7952 - val_accuracy: 0.2783 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 41/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8614 - accuracy: 0.7979 - val_loss: 3.3080 - val_accuracy: 0.2010 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 42/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9653 - accuracy: 0.7642 - val_loss: 6.0477 - val_accuracy: 0.1007 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 43/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.9426 - accuracy: 0.7768 - val_loss: 1.0020 - val_accuracy: 0.7787 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 44/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.9079 - accuracy: 0.7897 - val_loss: 2.5048 - val_accuracy: 0.3990 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 45/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8963 - accuracy: 0.7917 - val_loss: 2.3891 - val_accuracy: 0.4451 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 46/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8862 - accuracy: 0.7963 - val_loss: 3.0943 - val_accuracy: 0.1388 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 47/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.8632 - accuracy: 0.8033 - val_loss: 2.7453 - val_accuracy: 0.2920 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 48/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8568 - accuracy: 0.8039 - val_loss: 1.0408 - val_accuracy: 0.7569 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 49/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8669 - accuracy: 0.7989 - val_loss: 2.4406 - val_accuracy: 0.4124 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 50/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.8496 - accuracy: 0.8073 - val_loss: 2.7280 - val_accuracy: 0.3294 - lr: 0.1000\n",
      "Learning rate: 0.1\n",
      "Epoch 51/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.8542 - accuracy: 0.8059 - val_loss: 27.2717 - val_accuracy: 0.1000 - lr: 0.1000\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 52/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.7933 - accuracy: 0.8251 - val_loss: 0.7577 - val_accuracy: 0.8432 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 53/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.7068 - accuracy: 0.8543 - val_loss: 0.6258 - val_accuracy: 0.8864 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 54/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6791 - accuracy: 0.8616 - val_loss: 0.6707 - val_accuracy: 0.8710 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 55/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6588 - accuracy: 0.8680 - val_loss: 0.6213 - val_accuracy: 0.8858 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 56/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6427 - accuracy: 0.8703 - val_loss: 0.6377 - val_accuracy: 0.8815 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 57/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6316 - accuracy: 0.8725 - val_loss: 0.5898 - val_accuracy: 0.8963 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 58/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.6214 - accuracy: 0.8774 - val_loss: 0.5815 - val_accuracy: 0.8936 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 59/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6086 - accuracy: 0.8806 - val_loss: 0.5921 - val_accuracy: 0.8915 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 60/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5966 - accuracy: 0.8826 - val_loss: 0.5723 - val_accuracy: 0.8955 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 61/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5910 - accuracy: 0.8848 - val_loss: 0.5752 - val_accuracy: 0.8906 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 62/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5858 - accuracy: 0.8846 - val_loss: 0.5877 - val_accuracy: 0.8870 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 63/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5758 - accuracy: 0.8867 - val_loss: 0.5792 - val_accuracy: 0.8896 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 64/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5710 - accuracy: 0.8868 - val_loss: 0.5607 - val_accuracy: 0.8973 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 65/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5642 - accuracy: 0.8884 - val_loss: 0.5535 - val_accuracy: 0.8988 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 66/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5565 - accuracy: 0.8893 - val_loss: 0.5574 - val_accuracy: 0.8970 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 67/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5479 - accuracy: 0.8920 - val_loss: 0.5459 - val_accuracy: 0.8993 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 68/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5416 - accuracy: 0.8933 - val_loss: 0.5719 - val_accuracy: 0.8915 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 69/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5352 - accuracy: 0.8952 - val_loss: 0.5577 - val_accuracy: 0.8938 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 70/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5297 - accuracy: 0.8944 - val_loss: 0.5289 - val_accuracy: 0.9007 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 71/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5244 - accuracy: 0.8960 - val_loss: 0.6415 - val_accuracy: 0.8672 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 72/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5202 - accuracy: 0.8971 - val_loss: 0.5292 - val_accuracy: 0.8992 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 73/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5116 - accuracy: 0.8986 - val_loss: 0.5238 - val_accuracy: 0.8990 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 74/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.5115 - accuracy: 0.8972 - val_loss: 0.5256 - val_accuracy: 0.9025 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 75/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5056 - accuracy: 0.8983 - val_loss: 0.5215 - val_accuracy: 0.9016 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 76/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.5007 - accuracy: 0.8996 - val_loss: 0.5169 - val_accuracy: 0.9016 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 77/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4956 - accuracy: 0.9007 - val_loss: 0.5461 - val_accuracy: 0.8913 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 78/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4916 - accuracy: 0.9013 - val_loss: 0.5115 - val_accuracy: 0.9004 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 79/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4885 - accuracy: 0.9013 - val_loss: 0.5099 - val_accuracy: 0.9008 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 80/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4831 - accuracy: 0.9016 - val_loss: 0.5161 - val_accuracy: 0.8989 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 81/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4819 - accuracy: 0.9002 - val_loss: 0.5169 - val_accuracy: 0.8984 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 82/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4715 - accuracy: 0.9035 - val_loss: 0.5011 - val_accuracy: 0.9010 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 83/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4698 - accuracy: 0.9038 - val_loss: 0.5062 - val_accuracy: 0.9000 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 84/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4673 - accuracy: 0.9053 - val_loss: 0.5167 - val_accuracy: 0.8980 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 85/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4576 - accuracy: 0.9081 - val_loss: 0.5031 - val_accuracy: 0.9008 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 86/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4641 - accuracy: 0.9045 - val_loss: 0.5129 - val_accuracy: 0.8963 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 87/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4572 - accuracy: 0.9044 - val_loss: 0.5011 - val_accuracy: 0.9005 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 88/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4577 - accuracy: 0.9045 - val_loss: 0.4990 - val_accuracy: 0.8995 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 89/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4539 - accuracy: 0.9055 - val_loss: 0.4927 - val_accuracy: 0.9001 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 90/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4453 - accuracy: 0.9070 - val_loss: 0.5079 - val_accuracy: 0.8970 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 91/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4478 - accuracy: 0.9068 - val_loss: 0.4949 - val_accuracy: 0.9021 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 92/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4443 - accuracy: 0.9071 - val_loss: 0.5149 - val_accuracy: 0.8949 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 93/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4390 - accuracy: 0.9082 - val_loss: 0.5047 - val_accuracy: 0.8971 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 94/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4392 - accuracy: 0.9089 - val_loss: 0.4911 - val_accuracy: 0.9004 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 95/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4388 - accuracy: 0.9074 - val_loss: 0.4870 - val_accuracy: 0.8992 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 96/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4328 - accuracy: 0.9092 - val_loss: 0.4937 - val_accuracy: 0.8991 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 97/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4296 - accuracy: 0.9099 - val_loss: 0.4888 - val_accuracy: 0.8994 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 98/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4285 - accuracy: 0.9092 - val_loss: 0.5204 - val_accuracy: 0.8895 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 99/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4247 - accuracy: 0.9102 - val_loss: 0.4991 - val_accuracy: 0.9001 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 100/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4233 - accuracy: 0.9102 - val_loss: 0.5281 - val_accuracy: 0.8876 - lr: 0.0100\n",
      "Learning rate: 0.010000000000000002\n",
      "Epoch 101/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.4250 - accuracy: 0.9085 - val_loss: 0.4836 - val_accuracy: 0.9017 - lr: 0.0100\n",
      "Learning rate: 0.001\n",
      "Epoch 102/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.4034 - accuracy: 0.9173 - val_loss: 0.4494 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 103/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3878 - accuracy: 0.9245 - val_loss: 0.4472 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 104/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3819 - accuracy: 0.9248 - val_loss: 0.4466 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 105/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3760 - accuracy: 0.9275 - val_loss: 0.4468 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 106/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3761 - accuracy: 0.9276 - val_loss: 0.4404 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 107/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3762 - accuracy: 0.9253 - val_loss: 0.4402 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 108/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3709 - accuracy: 0.9293 - val_loss: 0.4430 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 109/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3668 - accuracy: 0.9291 - val_loss: 0.4438 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 110/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3653 - accuracy: 0.9314 - val_loss: 0.4426 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 111/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3674 - accuracy: 0.9287 - val_loss: 0.4419 - val_accuracy: 0.9123 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 112/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3677 - accuracy: 0.9281 - val_loss: 0.4414 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 113/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3643 - accuracy: 0.9304 - val_loss: 0.4435 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 114/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3611 - accuracy: 0.9320 - val_loss: 0.4429 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 115/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3601 - accuracy: 0.9317 - val_loss: 0.4450 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 116/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3560 - accuracy: 0.9327 - val_loss: 0.4437 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 117/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3572 - accuracy: 0.9313 - val_loss: 0.4420 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 118/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3578 - accuracy: 0.9318 - val_loss: 0.4432 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 119/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3593 - accuracy: 0.9313 - val_loss: 0.4414 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 120/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3570 - accuracy: 0.9320 - val_loss: 0.4440 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 121/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3531 - accuracy: 0.9335 - val_loss: 0.4429 - val_accuracy: 0.9133 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 122/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3549 - accuracy: 0.9338 - val_loss: 0.4426 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 123/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3518 - accuracy: 0.9330 - val_loss: 0.4421 - val_accuracy: 0.9131 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 124/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3499 - accuracy: 0.9334 - val_loss: 0.4441 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 125/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3523 - accuracy: 0.9335 - val_loss: 0.4446 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 126/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3476 - accuracy: 0.9364 - val_loss: 0.4445 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 127/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3518 - accuracy: 0.9345 - val_loss: 0.4447 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 128/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3472 - accuracy: 0.9364 - val_loss: 0.4411 - val_accuracy: 0.9131 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 129/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3508 - accuracy: 0.9349 - val_loss: 0.4423 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 130/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3504 - accuracy: 0.9331 - val_loss: 0.4437 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 131/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3477 - accuracy: 0.9340 - val_loss: 0.4421 - val_accuracy: 0.9119 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 132/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3479 - accuracy: 0.9348 - val_loss: 0.4414 - val_accuracy: 0.9125 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 133/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3426 - accuracy: 0.9372 - val_loss: 0.4450 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 134/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3429 - accuracy: 0.9356 - val_loss: 0.4431 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 135/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3452 - accuracy: 0.9351 - val_loss: 0.4433 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 136/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3424 - accuracy: 0.9351 - val_loss: 0.4436 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 137/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3393 - accuracy: 0.9367 - val_loss: 0.4428 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 138/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3419 - accuracy: 0.9358 - val_loss: 0.4427 - val_accuracy: 0.9125 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 139/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3421 - accuracy: 0.9358 - val_loss: 0.4441 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 140/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3411 - accuracy: 0.9350 - val_loss: 0.4473 - val_accuracy: 0.9092 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 141/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3426 - accuracy: 0.9356 - val_loss: 0.4453 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 142/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3397 - accuracy: 0.9367 - val_loss: 0.4445 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 143/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3399 - accuracy: 0.9370 - val_loss: 0.4424 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 144/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3403 - accuracy: 0.9361 - val_loss: 0.4410 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 145/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3385 - accuracy: 0.9364 - val_loss: 0.4407 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 146/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3334 - accuracy: 0.9386 - val_loss: 0.4416 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 147/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3349 - accuracy: 0.9369 - val_loss: 0.4381 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 148/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3320 - accuracy: 0.9382 - val_loss: 0.4401 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 149/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3356 - accuracy: 0.9363 - val_loss: 0.4406 - val_accuracy: 0.9115 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 150/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3298 - accuracy: 0.9395 - val_loss: 0.4419 - val_accuracy: 0.9113 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 151/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3355 - accuracy: 0.9369 - val_loss: 0.4432 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 152/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3364 - accuracy: 0.9363 - val_loss: 0.4410 - val_accuracy: 0.9121 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 153/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3324 - accuracy: 0.9384 - val_loss: 0.4450 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 154/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3283 - accuracy: 0.9404 - val_loss: 0.4444 - val_accuracy: 0.9109 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 155/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3317 - accuracy: 0.9379 - val_loss: 0.4419 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 156/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3306 - accuracy: 0.9386 - val_loss: 0.4407 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 157/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3324 - accuracy: 0.9383 - val_loss: 0.4411 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 158/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3312 - accuracy: 0.9383 - val_loss: 0.4440 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 159/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3305 - accuracy: 0.9393 - val_loss: 0.4443 - val_accuracy: 0.9109 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 160/180\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.3299 - accuracy: 0.9393 - val_loss: 0.4450 - val_accuracy: 0.9104 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 161/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3280 - accuracy: 0.9397 - val_loss: 0.4435 - val_accuracy: 0.9115 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 162/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3269 - accuracy: 0.9401 - val_loss: 0.4441 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 163/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3283 - accuracy: 0.9378 - val_loss: 0.4437 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 164/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3234 - accuracy: 0.9411 - val_loss: 0.4412 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 165/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3288 - accuracy: 0.9404 - val_loss: 0.4405 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 166/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3256 - accuracy: 0.9399 - val_loss: 0.4439 - val_accuracy: 0.9131 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 167/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3275 - accuracy: 0.9386 - val_loss: 0.4435 - val_accuracy: 0.9109 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 168/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3262 - accuracy: 0.9392 - val_loss: 0.4409 - val_accuracy: 0.9117 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 169/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3229 - accuracy: 0.9401 - val_loss: 0.4432 - val_accuracy: 0.9129 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 170/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3221 - accuracy: 0.9405 - val_loss: 0.4451 - val_accuracy: 0.9127 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 171/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3245 - accuracy: 0.9400 - val_loss: 0.4436 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 172/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3224 - accuracy: 0.9409 - val_loss: 0.4463 - val_accuracy: 0.9127 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 173/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3218 - accuracy: 0.9393 - val_loss: 0.4454 - val_accuracy: 0.9121 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 174/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3201 - accuracy: 0.9413 - val_loss: 0.4428 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 175/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3170 - accuracy: 0.9418 - val_loss: 0.4460 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 176/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3215 - accuracy: 0.9403 - val_loss: 0.4402 - val_accuracy: 0.9133 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 177/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3172 - accuracy: 0.9422 - val_loss: 0.4487 - val_accuracy: 0.9113 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 178/180\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.3165 - accuracy: 0.9424 - val_loss: 0.4412 - val_accuracy: 0.9137 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 179/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3153 - accuracy: 0.9426 - val_loss: 0.4439 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate: 0.001\n",
      "Epoch 180/180\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.3191 - accuracy: 0.9415 - val_loss: 0.4439 - val_accuracy: 0.9141 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f991f6d1e20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_scheduler = LearningRateScheduler(learning_rate_schedule)\n",
    "callbacks = [WandbCallback(), lr_scheduler]\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(width_shift_range=4,\n",
    "                             height_shift_range=4,\n",
    "                             horizontal_flip=True,\n",
    "                             preprocessing_function=get_random_eraser(p=1, pixel_level=True))\n",
    "datagen.fit(x_train)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs, workers=4,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc96b009-941c-4381-a259-f3f45619bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f19e946-b5c5-48d8-90e6-1144be24915b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
