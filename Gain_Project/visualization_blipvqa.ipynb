{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd7f6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BlipForQuestionAnswering were not initialized from the model checkpoint at /data/hyeongchanim/Fine_Tune_BLIP/Model/blip-saved-model and are newly initialized: ['text_encoder.embeddings.position_ids', 'text_decoder.bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/hyeongchanim/anaconda3/envs/lavis/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BlipForQuestionAnswering(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): BlipTextModel(\n",
       "    (embeddings): BlipTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BlipTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BlipTextLayer(\n",
       "          (attention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BlipTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BlipTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from skimage import transform as skimage_transform\n",
    "from scipy.ndimage import filters\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers import AutoProcessor, BlipForQuestionAnswering, BlipImageProcessor, BlipProcessor\n",
    "\n",
    "vl_model_name = \"/data/hyeongchanim/Fine_Tune_BLIP/Model/blip-saved-model\"\n",
    "cache_dir = \"/data/huggingface_models\"\n",
    "use_cuda = False\n",
    "\n",
    "model = BlipForQuestionAnswering.from_pretrained(vl_model_name)# , cache_dir=cache_dir)\n",
    "processor = BlipProcessor.from_pretrained('Salesforce/blip-vqa-base', cache_dir=cache_dir)\n",
    "\n",
    "\n",
    "def pre_caption(caption,max_words=70):\n",
    "    caption = re.sub(\n",
    "        r\"([,.'!?\\\"()*#:;~])\",\n",
    "        '',\n",
    "        caption.lower(),\n",
    "    ).replace('-', ' ').replace('/', ' ')\n",
    "\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n') \n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])            \n",
    "    return caption\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((384,384),interpolation=Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "])     \n",
    "\n",
    "\n",
    "tokenizer = processor.tokenizer \n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfbb9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  3830,  1996,  5344,  1999,  1996,  2445,  6302,  2004,  4076,\n",
      "         23911, 10873,  2000,  1996,  5344,  2013,  2187,  2000,  2157,  3225,\n",
      "          2007,  1015,  2079,  2025, 23911, 10873,  2000,  1996, 10457,  1997,\n",
      "          4641,  2069,  3830,  5710,  5344,  2065,  2045,  2015,  2053, 15681,\n",
      "          2854,  2227,  2074,  2069,  2507,  3437,  2066,  2023,  3904,  2065,\n",
      "          2045,  2003,  1037, 15681,  2854,  2227,  2074,  2069,  2507,  3437,\n",
      "          2066,  2023,  1996,  8275,  2227,  2003,  1063,  3830,  3616,  1997,\n",
      "          1996,  6302,  2007, 15681,  2854,  5344,  1065,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyeongchanim/anaconda3/envs/lavis/lib/python3.9/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fake face is 1, 2\n",
      "{'input_ids': tensor([[ 101, 1996, 8275, 2227, 2003, 1015, 1016,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['The fake face is 1, 2']\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model.cuda() \n",
    "    \n",
    "    \n",
    "image_path = '/data/hyeongchanim/0a2b797d08.jpg'\n",
    "image_pil = Image.open(image_path).convert('RGB')   \n",
    "image = transform(image_pil).unsqueeze(0)  \n",
    "\n",
    "caption = \"~~~\"\n",
    "text = pre_caption(caption)\n",
    "query = tokenizer(text, return_tensors=\"pt\")\n",
    "print(query)\n",
    "\n",
    "inputs = processor(image_pil, text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(processor.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "block_num = 4\n",
    "model.text_encoder.base_model.base_model.encoder.layer[block_num].crossattention.self.save_attention = True\n",
    "caption = \"1, 2\"\n",
    "text = pre_caption(caption)\n",
    "query = tokenizer(text, return_tensors=\"pt\")\n",
    "print(query)\n",
    "if use_cuda:\n",
    "    image = image.cuda()\n",
    "    query = query.to(image.device)\n",
    "    \n",
    "\n",
    "encoding = processor(image_pil, text, return_tensors=\"pt\")       \n",
    "\n",
    "answer = ['1, 2']\n",
    "print(answer)\n",
    "labels = processor.tokenizer(answer, return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(**encoding, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc8355df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlipForQuestionAnswering(\n",
       "  (vision_model): BlipVisionModel(\n",
       "    (embeddings): BlipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): BlipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BlipEncoderLayer(\n",
       "          (self_attn): BlipAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (projection): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): BlipMLP(\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (text_encoder): BlipTextModel(\n",
       "    (embeddings): BlipTextEmbeddings(\n",
       "      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BlipTextEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BlipTextLayer(\n",
       "          (attention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): BlipTextAttention(\n",
       "            (self): BlipTextSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): BlipTextSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BlipTextIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BlipTextOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_decoder): BlipTextLMHeadModel(\n",
       "    (bert): BlipTextModel(\n",
       "      (embeddings): BlipTextEmbeddings(\n",
       "        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): BlipTextEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BlipTextLayer(\n",
       "            (attention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BlipTextAttention(\n",
       "              (self): BlipTextSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): BlipTextSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BlipTextIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BlipTextOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BlipTextOnlyMLMHead(\n",
       "      (predictions): BlipTextLMPredictionHead(\n",
       "        (transform): BlipTextPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4680ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for i in range(12):\n",
    "    block_num = i\n",
    "    target_layer = model.text_encoder.base_model.base_model.encoder.layer[block_num].crossattention\n",
    "    target_layer.self.save_attention = True\n",
    "    labels = processor.tokenizer(answer, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    outputs = model(**encoding, labels=labels)\n",
    "\n",
    "    loss = outputs[0]\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        mask = query.attention_mask.view(query.attention_mask.size(0),1,-1,1,1) # (bsz, 1, token_len, 1, 1)\n",
    "        token_length = query.attention_mask.sum(dim=-1) - 2\n",
    "        token_length = token_length.cpu()\n",
    "        # grads and cams [bsz, num_head, seq_len, image_patch] = [1, 12, -1, 24*24]\n",
    "        grads=target_layer.self.get_attn_gradients()\n",
    "        grads=F.relu(grads)\n",
    "        cams=target_layer.self.get_attention_map()\n",
    "        cams=F.relu(cams)\n",
    "\n",
    "        cams = cams[:, :, :, 1:].reshape(image.size(0), 12, -1, 24, 24) * mask\n",
    "        grads = grads[:, :, :, 1:].clamp(0).reshape(image.size(0), 12, -1, 24, 24) * mask\n",
    "\n",
    "        gradcams = cams * grads\n",
    "        for ind in range(image.size(0)):\n",
    "            token_length_ = token_length[ind]\n",
    "            gradcam = gradcams[ind].mean(0).cpu().detach()\n",
    "            # [enc token gradcam, average gradcam across token, gradcam for individual token]\n",
    "            gradcam = torch.cat(\n",
    "                (\n",
    "                    gradcam[0:1, :],\n",
    "                    gradcam[1 : token_length_ + 1, :].sum(dim=0, keepdim=True)\n",
    "                    / token_length_,\n",
    "                    gradcam[1:, :],\n",
    "                )\n",
    "            )\n",
    "            gradcams = gradcam\n",
    "            \n",
    "    def getAttMap(img, attMap, blur=True, overlap=True):\n",
    "        # If attMap is a torch tensor, convert it to numpy array\n",
    "        if isinstance(attMap, torch.Tensor):\n",
    "            attMap = attMap.detach().cpu().numpy()\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu().numpy()\n",
    "            \n",
    "        attMap -= attMap.min()\n",
    "        if attMap.max() > 0:\n",
    "            attMap /= attMap.max()\n",
    "        attMap = skimage_transform.resize(attMap, img.shape[:2], order=3, mode='constant')\n",
    "        if blur:\n",
    "            attMap = filters.gaussian_filter(attMap, 0.02*max(img.shape[:2]))\n",
    "            attMap -= attMap.min()\n",
    "            attMap /= attMap.max()\n",
    "        cmap = plt.get_cmap('jet')\n",
    "        attMapV = cmap(attMap)\n",
    "        attMapV = np.delete(attMapV, 3, 2)\n",
    "        if overlap:\n",
    "            attMap = 1*(1-attMap**0.7).reshape(attMap.shape + (1,))*img + (attMap**0.7).reshape(attMap.shape+(1,)) * attMapV\n",
    "        return attMap\n",
    "\n",
    "\n",
    "    num_image = len(query.input_ids[0]) \n",
    "    fig, ax = plt.subplots(num_image, 1, figsize=(15,5*num_image))\n",
    "\n",
    "    rgb_image = cv2.imread(image_path)[:, :, ::-1]\n",
    "    rgb_image = np.float32(rgb_image) / 255\n",
    "\n",
    "    ax[0].imshow(rgb_image)\n",
    "    ax[0].set_yticks([])\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_xlabel(\"Image\")\n",
    "                \n",
    "    for i,token_id in enumerate(query.input_ids[0][-3:-1]):\n",
    "        word = tokenizer.decode([token_id])\n",
    "        gradcam_image = getAttMap(rgb_image, gradcam[i+1])\n",
    "        ax[i+1].imshow(gradcam_image)\n",
    "        ax[i+1].set_yticks([])\n",
    "        ax[i+1].set_xticks([])\n",
    "        ax[i+1].set_xlabel(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e948ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
